{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "helium_JAX.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ry7RIBV1HAMm",
        "wdwtL4kuerSF"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gcassella/NN-VMC/blob/main/helium_JAX.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fm9-6jxUCxO8"
      },
      "source": [
        "import jax.numpy as np\n",
        "import jax\n",
        "from jax import random, grad, jacfwd, jacrev, vmap, jit, pmap\n",
        "from jax.ops import index_add, index_update\n",
        "from functools import partial\n",
        "\n",
        "key = random.PRNGKey(0)\n",
        "key, subkey = random.split(key)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hz1WbP_xCxPJ"
      },
      "source": [
        "# Stochastic reconfiguration w/ Hylleraas wavefunction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMcJkpscE8hT"
      },
      "source": [
        "class Wavefunction():\n",
        "  def __init__(self, f, p0):\n",
        "    self.f = f\n",
        "    self.p = p0\n",
        "\n",
        "    self.hess = jacfwd(jacrev(lambda x: self.f(x, self.p), 0), 0)\n",
        "    self.p_grad = grad(self.f, 1)\n",
        "    self.p_gradlog = grad(lambda x, p: np.log(self.f(x, p)), 1)\n",
        "\n",
        "    # Cache evaluations to speed up?\n",
        "    self.p_gradlog_eval = jit(lambda x: self.p_gradlog(x, self.p))\n",
        "    self.p_grad_eval = jit(lambda x: self.p_grad(x, self.p))\n",
        "    self.lapl_eval = jit(lambda x: np.trace(self.hess(x).reshape(x.shape[0]*x.shape[1], x.shape[0]*x.shape[1])))\n",
        "    self.eval = jit(lambda x: self.f(x, self.p))\n",
        "    self.pdf_eval = jit(lambda x: np.power(np.abs(self.eval(x)), 2))\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7vBXf1kCxPV"
      },
      "source": [
        "@jit\n",
        "def hirschfelder_f(x, p):\n",
        "    r = np.linalg.norm(x, axis=1)\n",
        "    r1 = r[0]\n",
        "    r2 = r[1]\n",
        "\n",
        "    s = r1 + r2\n",
        "    t = r1 - r2\n",
        "    u = np.linalg.norm(np.subtract(x[1], x[0]))\n",
        "\n",
        "    return np.exp(-2*s)*(1 + 0.5*u*np.exp(-p[0]*u))*(1 + p[1]*s*u + p[2]*np.power(t, 2) + p[3]*np.power(u, 2))\n",
        "\n",
        "hirschfelder = Wavefunction(hirschfelder_f, np.array([1.0, 0.5, 0.5, -0.1]))\n",
        "\n",
        "@jit\n",
        "def simple_f(x, p):\n",
        "    r = np.linalg.norm(x, axis=1)\n",
        "    r1 = r[0]\n",
        "    r2 = r[1]\n",
        "\n",
        "    return np.exp(-p[0]*(r1 + r2))\n",
        "\n",
        "simple = Wavefunction(simple_f, np.array([2.0]))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sf66BnnICxPg"
      },
      "source": [
        "@partial(jit, static_argnums=(1,))\n",
        "def config_step(key, wf, config, config_prob, config_idx, step_size):\n",
        "    key, subkey = random.split(key)\n",
        "    move_proposal = random.normal(key, shape=(config.shape[1],))*step_size\n",
        "    proposal = index_add(config, config_idx%config.shape[0], move_proposal)\n",
        "    proposal_prob = wf.pdf_eval(proposal)\n",
        "\n",
        "    uniform = random.uniform(subkey)\n",
        "    accept = uniform < (proposal_prob / config_prob)\n",
        "\n",
        "    new_config = np.where(accept, proposal, config)\n",
        "    config_prob = np.where(accept, proposal_prob, config_prob)\n",
        "    return new_config, config_prob, config_idx+1\n",
        "\n",
        "@partial(jit, static_argnums=(1, 2, 3, 4))\n",
        "def get_configs(key, wf, n_iter, n_equi, step_size, initial_config):\n",
        "    \"\"\"\n",
        "    Carries out Metropolis-Hastings sampling according to the distribution |`wf`|**2.0.\n",
        "    \n",
        "    Performs `n_equi` equilibriation steps and `n_iter` sampling steps.\n",
        "    \"\"\"\n",
        "    \n",
        "    def mh_update(i, state):\n",
        "      key, config, prob, idx = state\n",
        "      _, key = random.split(key)\n",
        "      new_config, new_prob, new_idx = config_step(\n",
        "          key,\n",
        "          wf,\n",
        "          config,\n",
        "          prob,\n",
        "          idx,\n",
        "          step_size\n",
        "      )\n",
        "      return (key, new_config, new_prob, new_idx)\n",
        "\n",
        "    def mh_update_and_store(i, state):\n",
        "      key, config, prob, idx, configs = state\n",
        "      _, key = random.split(key)\n",
        "      new_config, new_prob, new_idx = config_step(\n",
        "          key,\n",
        "          wf,\n",
        "          config,\n",
        "          prob,\n",
        "          idx,\n",
        "          step_size\n",
        "      )\n",
        "      new_configs = index_update(configs, idx, new_config)\n",
        "      return (key, new_config, new_prob, new_idx, new_configs)\n",
        "\n",
        "    prob = wf.pdf_eval(initial_config)\n",
        "    key, config, prob, idx = jax.lax.fori_loop(0, n_equi, mh_update, (key, initial_config, prob, 0))\n",
        "    init_configs = np.zeros((n_iter, *initial_config.shape))\n",
        "    key, config, prob, idx, configs = jax.lax.fori_loop(0, n_iter, mh_update_and_store, (key, config, prob, 0, init_configs))\n",
        "\n",
        "    return configs"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaWNad0_CxPr"
      },
      "source": [
        "@partial(jit, static_argnums=(1,))\n",
        "def itime_hamiltonian(config, wf, tau=0.01):\n",
        "    n_electron = config.shape[0]\n",
        "    curr_wf = wf.eval(config)\n",
        "    acc = 0\n",
        "    # Calculate kinetic energy\n",
        "    acc += -0.5*(1/curr_wf)*wf.lapl_eval(config)\n",
        "    # Calculate electron-electron energy\n",
        "    for i in range(n_electron):\n",
        "        for j in range(n_electron):\n",
        "            if i < j:\n",
        "                acc += 1 / np.linalg.norm(np.subtract(config[i], config[j]))\n",
        "\n",
        "    # Calculate electron-nucleus energy, assume z=ne FOR NOW\n",
        "    for i in range(n_electron):\n",
        "        acc -= n_electron / np.linalg.norm(config[i])\n",
        "    # Forget about nucleus - nucleus energy FOR NOW\n",
        "\n",
        "    return 1-tau*acc\n",
        "\n",
        "@partial(jit, static_argnums=(1,))\n",
        "def sr_op(config, wf):\n",
        "    gradlog = np.concatenate((np.array([1]), np.array(wf.p_gradlog_eval(config))))\n",
        "    ih = itime_hamiltonian(config, wf)\n",
        "    \n",
        "    return np.multiply(gradlog, ih)\n",
        "\n",
        "@partial(jit, static_argnums=(1,))\n",
        "def overlap_matrix(config, wf):\n",
        "    \"\"\"\n",
        "    Find the overlap matrix on the space of the parametric derivatives of `wf`\n",
        "    \"\"\"\n",
        "    \n",
        "    gradlog = np.concatenate((np.array([1]), np.array(wf.p_gradlog_eval(config))))\n",
        "    overlap_ij = vmap(lambda idx: gradlog[idx[0]]*gradlog[idx[1]])\n",
        "    \n",
        "    grid_pairs = np.array([(i,j) for i in range(gradlog.shape[0]) for j in range(gradlog.shape[0])])\n",
        "    \n",
        "    return overlap_ij(grid_pairs).reshape(gradlog.shape[0], gradlog.shape[0])\n",
        "\n",
        "@partial(jit, static_argnums=(1,))\n",
        "def local_energy(config, wf):\n",
        "    \"\"\"\n",
        "    Local energy operator. Uses JAX autograd to obtain laplacian for KE.\n",
        "    \"\"\"\n",
        "\n",
        "    n_electron = config.shape[0]\n",
        "    acc = 0\n",
        "    # Calculate kinetic energy\n",
        "    acc += -0.5*(1/wf.eval(config))*wf.lapl_eval(config)\n",
        "    # Calculate electron-electron energy\n",
        "    for i in range(n_electron):\n",
        "        for j in range(n_electron):\n",
        "            if i < j:\n",
        "                acc += 1 / np.linalg.norm(np.subtract(config[i], config[j]))\n",
        "\n",
        "    # Calculate electron-nucleus energy, assume z=ne FOR NOW\n",
        "    for i in range(n_electron):\n",
        "        acc -= n_electron / np.linalg.norm(config[i])\n",
        "\n",
        "    return acc"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGuqs5ZbCxPz"
      },
      "source": [
        "@partial(jit, static_argnums=(1,2,))\n",
        "def monte_carlo(configs, op, wf):\n",
        "    \"\"\"\n",
        "    Performs a Monte Carlo integration using the `configs` walker positions\n",
        "    of the expectation value of `op` for the wavefunction `wf`.\n",
        "    \n",
        "    Returns the expectation value, variance and a list of the sampled values {O_i}\n",
        "    \"\"\"\n",
        "\n",
        "    samp_rate = 100\n",
        "    walker_values = vmap(lambda config: op(config, wf))(configs)\n",
        "    op_output_shape = walker_values[0].shape\n",
        "    num_blocks = (walker_values.shape[0]//samp_rate)\n",
        "    blocks = walker_values[:samp_rate*(num_blocks)].reshape((num_blocks, samp_rate, *op_output_shape))\n",
        "    k = blocks.shape[0]\n",
        "    block_means = np.mean(blocks, axis=1)\n",
        "    op_expec = np.mean(block_means, axis=0)\n",
        "    op_var = 1/(k*(k-1))*np.sum(np.power(block_means - op_expec, 2), axis=0)\n",
        "    return op_expec, op_var"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvTT8u-Zj_UO"
      },
      "source": [
        "run_mcmc = vmap(get_configs, in_axes=(0, None, None, None, None, 0), out_axes=0)\n",
        "run_int = vmap(monte_carlo, in_axes=(0, None, None), out_axes=0)\n",
        "\n",
        "def reduce_mc_outs(outs):\n",
        "  k = outs[0].shape[0]\n",
        "  mean = np.mean(outs[0], axis=0)\n",
        "  variance = (1/k/(k-1))*np.sum(outs[1] + np.power(outs[0] - mean, 2), axis=0)\n",
        "  return mean, variance"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dxuEJZH6rQM"
      },
      "source": [
        "n_equi = 1000\n",
        "n_iter = 10000\n",
        "n_chains = 500\n",
        "xis = random.uniform(key, (n_chains, 2, 3))\n",
        "keys = random.split(key, n_chains)\n",
        "configs = run_mcmc(keys, simple, n_iter, n_equi, 0.5, xis)\n",
        "E_E, E_V = reduce_mc_outs(run_int(configs, local_energy, simple))\n",
        "overlap_E, overlap_V = reduce_mc_outs(run_int(configs, overlap_matrix, simple))\n",
        "#sr_E, sr_V = reduce_mc_outs(run_int(configs, sr_op, simple))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fRgxAz1DYKN",
        "outputId": "70f3fe1c-118e-4f13-a9e6-04e0354ff169",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "E_E"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray(-2.7496672, dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ry7RIBV1HAMm"
      },
      "source": [
        "## Simple WF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6V9vc3hiC-Qy",
        "outputId": "78e7d689-8df0-48e3-95c6-288f1f218cc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "source": [
        "key = random.PRNGKey(0)\n",
        "n_equi = 10000\n",
        "n_iter = 100000\n",
        "n_chains = 500\n",
        "xis = random.uniform(key, (n_chains, 2, 3))\n",
        "keys = random.split(key, n_chains)\n",
        "simple = Wavefunction(simple_f, np.array([2.0]))\n",
        "vals = [np.array(2.0)]\n",
        "\n",
        "for i in range(40):\n",
        "  configs = run_mcmc(keys, simple, n_iter, n_equi, 0.5, xis)\n",
        "  E_E, E_V = reduce_mc_outs(run_int(configs, local_energy, simple))\n",
        "  overlap_E, overlap_V = reduce_mc_outs(run_int(configs, overlap_matrix, simple))\n",
        "  sr_E, sr_V = reduce_mc_outs(run_int(configs, sr_op, simple))\n",
        "\n",
        "  dps = np.linalg.solve(overlap_E, sr_E)\n",
        "  p0 = np.add(simple.p, dps[1:] / dps[0])\n",
        "  # VERY IMPORTANT NOTE: JAX will not re-jit the operators if Wavefunction.p\n",
        "  # is updated internally by, e.g., a getter or setter. I don't know how to solve\n",
        "  # this problem currently aside from simply reinstantiating Wavefunction each\n",
        "  # time Wavefunction.p needs to be changed\n",
        "  #\n",
        "  # Perhaps this isn't such an issue if one sticks with a purely functional style\n",
        "  # and uses classes like immutable structs?\n",
        "  simple = Wavefunction(simple_f, p0)\n",
        "  vals.append(p0)\n",
        "  print(p0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.9276963]\n",
            "[1.8775854]\n",
            "[1.840691]\n",
            "[1.8123969]\n",
            "[1.7903057]\n",
            "[1.7726845]\n",
            "[1.7584919]\n",
            "[1.7468067]\n",
            "[1.7373891]\n",
            "[1.7295324]\n",
            "[1.7229042]\n",
            "[1.7173197]\n",
            "[1.712626]\n",
            "[1.7087704]\n",
            "[1.7055061]\n",
            "[1.702796]\n",
            "[1.700402]\n",
            "[1.6984478]\n",
            "[1.696852]\n",
            "[1.6954029]\n",
            "[1.6942947]\n",
            "[1.6933272]\n",
            "[1.6924536]\n",
            "[1.6917834]\n",
            "[1.6911279]\n",
            "[1.690638]\n",
            "[1.6901832]\n",
            "[1.6898696]\n",
            "[1.6895639]\n",
            "[1.689298]\n",
            "[1.6891017]\n",
            "[1.6889309]\n",
            "[1.6887196]\n",
            "[1.6885333]\n",
            "[1.6884184]\n",
            "[1.6882296]\n",
            "[1.6881112]\n",
            "[1.6879984]\n",
            "[1.6878452]\n",
            "[1.6877065]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTMPooDtDb12",
        "outputId": "b9f667c9-1d5c-4dd1-bfbf-b6c59bed393d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "configs = run_mcmc(keys, simple, n_iter, n_equi, 0.5, xis)\n",
        "E_E, E_V = reduce_mc_outs(run_int(configs, local_energy, simple))\n",
        "print(\"Ground state energy {} pm {} after 20 iterations with parameter {}\".format(E_E, np.sqrt(E_V), p0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ground state energy -2.8464293479919434 pm 0.0006964870844967663 after 20 iterations with parameter [1.6877065]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mj6TQsqMYhAv"
      },
      "source": [
        "For reference the true minimum of $\\langle E\\rangle \\simeq -2.85 \\text{a. u.}$ of the simple wf ansatz occurs at $\\alpha$=1.6875"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdwtL4kuerSF"
      },
      "source": [
        "## Hirschfelder wavefunction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xhnt3LSetLC",
        "outputId": "bfeefb88-6f2c-4133-d103-c21e528bd91e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "source": [
        "n_equi = 10000\n",
        "n_iter = 100000\n",
        "n_chains = 100\n",
        "xis = random.uniform(key, (n_chains, 2, 3))\n",
        "keys = random.split(key, n_chains)\n",
        "hirschfelder = Wavefunction(hirschfelder_f, np.array([1.0, 0.5, 0.5, -0.1]))\n",
        "vals = [p0]\n",
        "\n",
        "for i in range(40):\n",
        "  configs = run_mcmc(keys, hirschfelder, n_iter, n_equi, 0.5, xis)\n",
        "  E_E, E_V = reduce_mc_outs(run_int(configs, local_energy, hirschfelder))\n",
        "  overlap_E, overlap_V = reduce_mc_outs(run_int(configs, overlap_matrix, hirschfelder))\n",
        "  sr_E, sr_V = reduce_mc_outs(run_int(configs, sr_op, hirschfelder))\n",
        "\n",
        "  dps = np.linalg.solve(overlap_E, sr_E)\n",
        "  p0 = np.add(hirschfelder.p, dps[1:] / dps[0])\n",
        "  hirschfelder = Wavefunction(hirschfelder_f, p0)\n",
        "  vals.append(p0)\n",
        "  print(p0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 1.4236624   0.5324303   0.44780415 -0.1185777 ]\n",
            "[ 1.8274268   0.52555925  0.38857707 -0.14034285]\n",
            "[ 2.1532216   0.5014799   0.34327382 -0.14871532]\n",
            "[ 2.1948583   0.47658455  0.31286922 -0.14467369]\n",
            "[ 2.0115626   0.4537761   0.29153362 -0.13413884]\n",
            "[ 1.8770361   0.43206456  0.27365276 -0.12360602]\n",
            "[ 1.7592784   0.41211852  0.2586345  -0.11351466]\n",
            "[ 1.6649686   0.39407298  0.24570781 -0.10441782]\n",
            "[ 1.577496    0.37777448  0.23474768 -0.09616143]\n",
            "[ 1.5076677   0.3633409   0.22492754 -0.0890992 ]\n",
            "[ 1.4467105   0.35067847  0.21633679 -0.08313622]\n",
            "[ 1.3945475   0.33940798  0.2087992  -0.0780477 ]\n",
            "[ 1.3490429   0.32962883  0.20208855 -0.07381274]\n",
            "[ 1.3063763   0.3208327   0.19627199 -0.07010238]\n",
            "[ 1.2701937   0.31305918  0.1910933  -0.06695889]\n",
            "[ 1.2391235   0.3063077   0.18647681 -0.06442851]\n",
            "[ 1.2104979   0.30031046  0.18252607 -0.06231608]\n",
            "[ 1.187672    0.29506224  0.17885162 -0.06052488]\n",
            "[ 1.1666455   0.29016247  0.17578799 -0.05881443]\n",
            "[ 1.1456738   0.28589332  0.17293528 -0.0574354 ]\n",
            "[ 1.129263    0.28224057  0.17024308 -0.05635386]\n",
            "[ 1.1131905   0.27873302  0.16814847 -0.05524873]\n",
            "[ 1.0984658   0.27568334  0.1661749  -0.05435653]\n",
            "[ 1.0862108   0.2730122   0.16440171 -0.05358232]\n",
            "[ 1.0750093   0.27056193  0.16283806 -0.05288792]\n",
            "[ 1.0650601   0.26846287  0.16138089 -0.05233345]\n",
            "[ 1.0556324   0.26661938  0.16012001 -0.05189104]\n",
            "[ 1.049433    0.2649742   0.1589145  -0.05145911]\n",
            "[ 1.042331    0.26350895  0.15793398 -0.05111194]\n",
            "[ 1.0345564   0.262147    0.15706775 -0.05081553]\n",
            "[ 1.0291275   0.26089907  0.15629807 -0.05046355]\n",
            "[ 1.0244983   0.2598552   0.15557402 -0.0502286 ]\n",
            "[ 1.019854    0.2589599   0.15485553 -0.05004753]\n",
            "[ 1.0158389   0.25807545  0.1542707  -0.0498124 ]\n",
            "[ 1.0135514   0.25729942  0.1538427  -0.04958712]\n",
            "[ 1.010198    0.2565669   0.1534436  -0.04938283]\n",
            "[ 1.0065552   0.2558841   0.1530831  -0.04922125]\n",
            "[ 1.0040314   0.2554026   0.15273264 -0.04915685]\n",
            "[ 1.0028024   0.25498846  0.15240397 -0.04907146]\n",
            "[ 0.9998273   0.2546163   0.1520364  -0.04904639]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gr6FvtZFe7pu",
        "outputId": "cc805b0b-30b7-4e89-db4e-d3452a02ce91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "configs = run_mcmc(keys, hirschfelder, n_iter, n_equi, 0.5, xis)\n",
        "E_E, E_V = reduce_mc_outs(run_int(configs, local_energy, hirschfelder))\n",
        "print(\"Ground state energy {} pm {} after 40 iterations with parameter {}\".format(E_E, np.sqrt(E_V), p0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ground state energy -2.901717185974121 pm 0.00027340053929947317 after 40 iterations with parameter [ 0.9998273   0.2546163   0.1520364  -0.04904639]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKAYSNCrjTfO",
        "outputId": "98be4685-e9ec-4f1d-f1e5-e0bc83ef90b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"This {}mHa from the true ground state energy\".format(\n",
        "    np.abs(-2.903 - E_E)*1e3\n",
        "))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This 1.2829303741455078mHa from the true ground state energy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ampdYuMD_vB1"
      },
      "source": [
        "## Hirschfelder-type wavefunction with NNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYIUOcbIAI6c"
      },
      "source": [
        "$$\\psi=e^{-2\\left(r_{1}+r_{2}\\right)}\\left(1+\\frac{1}{2} r_{12} e^{-\\alpha r_{12}}\\right) g\\left(r_{1}, r_{2}, r_{12}\\right)$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZg7Ko5E_uPB"
      },
      "source": [
        "# A helper function to randomly initialize weights and biases\n",
        "# for a dense neural network layer\n",
        "def random_layer_params(m, n, key, scale=1):\n",
        "  w_key, b_key = random.split(key)\n",
        "  return [scale * random.normal(w_key, (n, m)), scale * random.normal(b_key, (n,))]\n",
        "\n",
        "# Initialize all layers for a fully-connected neural network with sizes \"sizes\"\n",
        "def init_network_params(sizes, key):\n",
        "  keys = random.split(key, len(sizes))\n",
        "  return [random_layer_params(m, n, k) for m, n, k in zip(sizes[:-1], sizes[1:], keys)]\n",
        "\n",
        "@jit\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def predict(x, params):\n",
        "  # per-example predictions\n",
        "  r = np.linalg.norm(x, axis=1)\n",
        "  r1 = r[0]\n",
        "  r2 = r[1]\n",
        "  u = np.linalg.norm(np.subtract(x[1], x[0]))\n",
        "\n",
        "  activations = np.array([r1, r2, u])\n",
        "  for w, b in params[:-1]:\n",
        "    outputs = np.dot(w, activations) + b\n",
        "    activations = tanh(outputs)\n",
        "  \n",
        "  final_w, final_b = params[-1]\n",
        "  outputs = np.dot(final_w, activations) + final_b\n",
        "  return outputs[0]\n",
        " \n",
        "layer_sizes = [3, 12, 12, 1]\n",
        "key, subkey = random.split(key)\n",
        "params = init_network_params(layer_sizes, key)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpwNLUTxL8gH"
      },
      "source": [
        "Pretrain params to match $g(r1,r2,r12) = \\left(1+0.2119 s u+0.1406 t^{2}-0.003 u^{2}\\right)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "np8F70iuMFSl",
        "outputId": "34d720f2-e473-4a53-b470-315f353452f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "def g(x):\n",
        "  r = np.linalg.norm(x, axis=1)\n",
        "  r1 = r[0]\n",
        "  r2 = r[1]\n",
        "\n",
        "  s = r1 + r2\n",
        "  t = r1 - r2\n",
        "  u = np.linalg.norm(np.subtract(x[1], x[0]))\n",
        "  return (1 + 0.2119*s*u + 0.1406*t**2.0 - 0.003*u**2.0)\n",
        "\n",
        "g(np.array([[2.0, 1.0, 1.0], [1.0,1.0,2.0]]))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray(2.4620862, dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GKewXcjR5qI"
      },
      "source": [
        "from jax.experimental import optimizers"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYMOMffP-0fO",
        "outputId": "9cf920b5-10c1-40ba-8c93-e2983c887d7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(params)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[DeviceArray([[ 0.17032038, -0.15212938,  0.4953925 ],\n",
            "             [ 0.55532193, -1.4711964 ,  0.00492484],\n",
            "             [ 1.8502427 ,  0.49203628,  1.61406   ],\n",
            "             [-1.4216027 ,  1.19387   , -0.23470964],\n",
            "             [-1.0294129 , -0.8314868 , -0.2624491 ],\n",
            "             [-2.29874   , -0.7720694 ,  0.68447214],\n",
            "             [-1.0402237 , -0.6654453 ,  0.21445496],\n",
            "             [ 0.19351044, -0.0628294 ,  0.67243254],\n",
            "             [ 0.12465691,  0.3315257 , -0.27397084],\n",
            "             [-0.08468974,  0.0333338 , -0.33523253],\n",
            "             [-1.5351669 ,  1.7267078 , -1.5075749 ],\n",
            "             [ 0.7374547 , -0.41872683,  0.21236524]], dtype=float32), DeviceArray([ 1.2887936 , -1.1984264 , -0.13047883, -0.13376504,\n",
            "              0.14090303,  1.2809039 , -0.882967  ,  0.38619992,\n",
            "             -0.43700072, -0.10178029, -0.40020615, -1.0684481 ],            dtype=float32)], [DeviceArray([[ 5.81776500e-01, -1.08965719e+00, -1.66196501e+00,\n",
            "              -4.35995281e-01, -3.88160884e-01,  2.23964834e+00,\n",
            "               6.56079575e-02,  1.01669669e+00,  5.37667155e-01,\n",
            "               1.27806544e+00,  7.24692047e-01,  1.43780720e+00],\n",
            "             [-7.54729748e-01, -6.13756120e-01, -3.96569759e-01,\n",
            "              -6.86420262e-01,  8.24264884e-01, -1.02780849e-01,\n",
            "              -1.92596531e+00,  2.36949578e-01,  2.76788616e+00,\n",
            "              -9.15109813e-01, -8.12007666e-01, -1.24255955e-01],\n",
            "             [ 4.11449909e-01,  1.55371204e-01, -1.73991639e-03,\n",
            "               5.40794253e-01,  3.42149109e-01, -7.09513009e-01,\n",
            "              -3.04076552e-01, -1.23314297e+00, -6.36116982e-01,\n",
            "               5.71848392e-01, -5.04568994e-01, -1.05284381e+00],\n",
            "             [-4.08511013e-01,  2.05388293e-01, -1.05468941e+00,\n",
            "              -5.65267205e-01, -9.39376295e-01,  1.47888684e+00,\n",
            "               1.63358882e-01, -2.12636828e+00,  1.11837995e+00,\n",
            "               7.37822235e-01, -2.34141126e-01,  6.21274948e-01],\n",
            "             [-1.05926347e+00,  3.64573985e-01,  1.11618161e+00,\n",
            "              -1.36241901e+00, -4.65231955e-01,  9.70061183e-01,\n",
            "              -2.56992847e-01, -1.32982686e-01,  4.44611087e-02,\n",
            "               5.38084626e-01,  2.44792372e-01, -7.77537346e-01],\n",
            "             [-1.83623701e-01, -7.73599684e-01,  1.97816443e+00,\n",
            "              -1.84228420e+00,  2.27306747e+00,  1.11722469e+00,\n",
            "              -2.98790306e-01, -6.76727071e-02, -4.58563596e-01,\n",
            "               1.19729209e+00,  6.49781585e-01, -9.85273123e-01],\n",
            "             [ 1.01765656e+00, -3.46743375e-01, -6.98683858e-01,\n",
            "               2.04176858e-01,  3.72804433e-01,  1.49258092e-01,\n",
            "               5.41055381e-01,  1.24782681e+00, -7.71897912e-01,\n",
            "               7.72640884e-01,  1.13681543e+00, -2.34305644e+00],\n",
            "             [-1.18390667e+00,  6.00497782e-01, -1.16320288e+00,\n",
            "              -6.52145147e-01,  3.70757431e-01,  8.14099252e-01,\n",
            "               5.86560890e-02,  7.88878649e-02, -8.15026641e-01,\n",
            "              -5.80611587e-01,  2.02673531e+00,  4.94417787e-01],\n",
            "             [ 8.06963503e-01,  1.98666036e-01,  1.37268528e-01,\n",
            "              -3.26468915e-01, -8.58686492e-02,  6.93790078e-01,\n",
            "              -7.31715977e-01, -4.55239743e-01,  4.47327435e-01,\n",
            "               2.25191921e-01, -3.13747069e-03, -1.14546442e+00],\n",
            "             [-5.80359042e-01, -7.25176454e-01,  1.45243868e-01,\n",
            "               2.97130823e-01, -2.24447846e-02,  9.82456446e-01,\n",
            "               7.70520687e-01,  8.86049211e-01,  6.62710011e-01,\n",
            "              -1.33528963e-01, -1.46439338e+00, -4.07501042e-01],\n",
            "             [-7.65585363e-01, -3.43969673e-01, -1.76877832e+00,\n",
            "              -4.93162930e-01, -9.26519275e-01,  4.55708325e-01,\n",
            "              -1.71282935e+00,  7.86327869e-02,  2.95402050e-01,\n",
            "              -4.78405088e-01, -5.64351022e-01,  2.98024118e-01],\n",
            "             [-4.47344750e-01, -6.25115216e-01,  1.61042392e-01,\n",
            "              -3.67599040e-01, -2.04379129e+00, -1.05798233e+00,\n",
            "              -6.30353034e-01,  1.52248338e-01, -1.01026368e+00,\n",
            "              -2.67726034e-01, -9.76236224e-01, -1.10918748e+00]],            dtype=float32), DeviceArray([-1.3096181 , -0.4835204 ,  1.3162483 ,  0.1630476 ,\n",
            "              1.5317078 , -2.2731047 , -0.35486007, -0.20880038,\n",
            "              1.5310235 , -1.2049345 , -1.4691826 , -2.1920664 ],            dtype=float32)], [DeviceArray([[ 0.39640704, -2.2268379 , -0.73386055,  1.0019883 ,\n",
            "              -0.5808422 , -0.44589636, -0.1564055 ,  0.03498421,\n",
            "              -1.0505106 , -0.02146115, -1.1018991 ,  0.66039264]],            dtype=float32), DeviceArray([1.7012464], dtype=float32)]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkuaUei8R9T0",
        "outputId": "247d0382-daad-4767-8fc1-8a5603e28448",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "batch_size = 1000\n",
        "opt_init, opt_update, get_params = optimizers.adam(step_size=1e-2)\n",
        "\n",
        "def loss(params, inputs, targets):\n",
        "    # Computes average loss for the batch\n",
        "    predictions = vmap(predict, in_axes=(0, None))(inputs, params)\n",
        "    return np.mean((targets - predictions)**2.0)\n",
        "\n",
        "opt_state = opt_init(params)\n",
        "\n",
        "@jit\n",
        "def step(i, opt_state, x1, y1):\n",
        "    p = get_params(opt_state)\n",
        "    v, g = jax.value_and_grad(loss)(p, x1, y1)\n",
        "    return v, opt_update(i, g, opt_state)\n",
        "\n",
        "for i in range(1000):\n",
        "  key, subkey = jax.random.split(key)\n",
        "  xis = random.uniform(key, (batch_size, 2, 3))\n",
        "  yis = vmap(g)(xis)\n",
        "             \n",
        "  v, opt_state = step(i, opt_state, xis, yis)\n",
        "  print(v)\n",
        "\n",
        "params = get_params(opt_state)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9622067\n",
            "0.57115227\n",
            "0.6202285\n",
            "0.5641536\n",
            "0.5099169\n",
            "0.35819966\n",
            "0.27679816\n",
            "0.27328286\n",
            "0.29929578\n",
            "0.25216475\n",
            "0.219412\n",
            "0.1625536\n",
            "0.15355973\n",
            "0.11677519\n",
            "0.10389205\n",
            "0.10031447\n",
            "0.093164496\n",
            "0.0881365\n",
            "0.07747473\n",
            "0.05603502\n",
            "0.041122366\n",
            "0.041176233\n",
            "0.040185124\n",
            "0.03940032\n",
            "0.03855387\n",
            "0.033953447\n",
            "0.032044124\n",
            "0.02646026\n",
            "0.023414023\n",
            "0.024791678\n",
            "0.024816183\n",
            "0.027833462\n",
            "0.025229642\n",
            "0.021677772\n",
            "0.023617515\n",
            "0.01920466\n",
            "0.020640142\n",
            "0.021336377\n",
            "0.023836236\n",
            "0.021429835\n",
            "0.020081155\n",
            "0.018534623\n",
            "0.018833501\n",
            "0.017895047\n",
            "0.016987296\n",
            "0.020895587\n",
            "0.018035853\n",
            "0.018343316\n",
            "0.017376853\n",
            "0.017063968\n",
            "0.015897049\n",
            "0.016556155\n",
            "0.01710117\n",
            "0.016812699\n",
            "0.015962474\n",
            "0.014042572\n",
            "0.014829695\n",
            "0.014093005\n",
            "0.01362275\n",
            "0.013545292\n",
            "0.012196322\n",
            "0.013471211\n",
            "0.0128430985\n",
            "0.012355731\n",
            "0.011464764\n",
            "0.013038701\n",
            "0.01049162\n",
            "0.012264994\n",
            "0.011119368\n",
            "0.011652776\n",
            "0.010454827\n",
            "0.011171997\n",
            "0.010785171\n",
            "0.009996867\n",
            "0.009987246\n",
            "0.00907038\n",
            "0.010144171\n",
            "0.009574603\n",
            "0.008720641\n",
            "0.008558175\n",
            "0.008256233\n",
            "0.008788521\n",
            "0.0074349474\n",
            "0.008565261\n",
            "0.008287347\n",
            "0.008347998\n",
            "0.008401364\n",
            "0.006968939\n",
            "0.0076739094\n",
            "0.008005726\n",
            "0.007850067\n",
            "0.0071623004\n",
            "0.0067056254\n",
            "0.006912859\n",
            "0.006605664\n",
            "0.0060326937\n",
            "0.0072360756\n",
            "0.0069444105\n",
            "0.007030161\n",
            "0.0063205827\n",
            "0.005625598\n",
            "0.006212903\n",
            "0.0055278074\n",
            "0.005702661\n",
            "0.005172455\n",
            "0.0052777743\n",
            "0.0055106203\n",
            "0.0052659856\n",
            "0.005717575\n",
            "0.0048306193\n",
            "0.005481834\n",
            "0.0059824022\n",
            "0.0052485503\n",
            "0.0048861783\n",
            "0.0047685737\n",
            "0.0053974926\n",
            "0.00491107\n",
            "0.004217584\n",
            "0.004541788\n",
            "0.0049004927\n",
            "0.0046360022\n",
            "0.004591492\n",
            "0.004517362\n",
            "0.004845962\n",
            "0.0048029977\n",
            "0.004351475\n",
            "0.004402756\n",
            "0.0042265486\n",
            "0.0046330313\n",
            "0.0037961528\n",
            "0.0036182897\n",
            "0.0042055845\n",
            "0.0041781054\n",
            "0.004048579\n",
            "0.003868966\n",
            "0.0034843653\n",
            "0.003581503\n",
            "0.0035276448\n",
            "0.0038859288\n",
            "0.0038331153\n",
            "0.0037707833\n",
            "0.0032937604\n",
            "0.0034181962\n",
            "0.0035896527\n",
            "0.0035497986\n",
            "0.0032996628\n",
            "0.0032862364\n",
            "0.0031965512\n",
            "0.0030205147\n",
            "0.003864175\n",
            "0.0029463528\n",
            "0.0033915779\n",
            "0.0040385947\n",
            "0.002823586\n",
            "0.003043831\n",
            "0.0028234834\n",
            "0.0029613492\n",
            "0.0028709031\n",
            "0.0028951718\n",
            "0.0031240685\n",
            "0.0027658346\n",
            "0.0029383996\n",
            "0.0025689835\n",
            "0.0028800617\n",
            "0.002781295\n",
            "0.0028933564\n",
            "0.0025195933\n",
            "0.0023942825\n",
            "0.0028509554\n",
            "0.0027049733\n",
            "0.0027333086\n",
            "0.0019946822\n",
            "0.0021842362\n",
            "0.002398811\n",
            "0.003004647\n",
            "0.0023742225\n",
            "0.0025500837\n",
            "0.0023281067\n",
            "0.0026863758\n",
            "0.0025322195\n",
            "0.002319153\n",
            "0.0023763038\n",
            "0.0021895547\n",
            "0.0024124442\n",
            "0.0026099172\n",
            "0.0018291227\n",
            "0.0019671826\n",
            "0.002083817\n",
            "0.0020445464\n",
            "0.0022057046\n",
            "0.0018896185\n",
            "0.0020332518\n",
            "0.00186717\n",
            "0.0018299468\n",
            "0.0019377167\n",
            "0.0019521505\n",
            "0.0018561942\n",
            "0.0016993997\n",
            "0.0017807806\n",
            "0.0020728954\n",
            "0.0019590035\n",
            "0.0018160263\n",
            "0.0016632945\n",
            "0.0018997694\n",
            "0.0016784673\n",
            "0.0016581398\n",
            "0.00149101\n",
            "0.0018053473\n",
            "0.001459328\n",
            "0.0018299896\n",
            "0.0015103918\n",
            "0.0015587126\n",
            "0.0018561285\n",
            "0.0019242099\n",
            "0.001865858\n",
            "0.0014936703\n",
            "0.0017356232\n",
            "0.001621593\n",
            "0.001684783\n",
            "0.0014299973\n",
            "0.0014969186\n",
            "0.0017509352\n",
            "0.0017824452\n",
            "0.0014182421\n",
            "0.0015427395\n",
            "0.0017774036\n",
            "0.0014029832\n",
            "0.0015968782\n",
            "0.0016279145\n",
            "0.0018490995\n",
            "0.0016936727\n",
            "0.0012441694\n",
            "0.0014605041\n",
            "0.0015986575\n",
            "0.0010739287\n",
            "0.0013679923\n",
            "0.0012745879\n",
            "0.0015107957\n",
            "0.0014763267\n",
            "0.001261677\n",
            "0.0011868043\n",
            "0.0012797449\n",
            "0.0013356381\n",
            "0.0013043176\n",
            "0.0010749426\n",
            "0.0013610472\n",
            "0.001045551\n",
            "0.0011929718\n",
            "0.001267731\n",
            "0.0012503118\n",
            "0.001208404\n",
            "0.0012172778\n",
            "0.0012330265\n",
            "0.0012253459\n",
            "0.00112599\n",
            "0.00111063\n",
            "0.0013705335\n",
            "0.0012108672\n",
            "0.001247289\n",
            "0.001282614\n",
            "0.0010989372\n",
            "0.0011406705\n",
            "0.0012304788\n",
            "0.001220465\n",
            "0.0012001375\n",
            "0.0010466609\n",
            "0.0010229814\n",
            "0.0014321499\n",
            "0.0010049351\n",
            "0.0012242025\n",
            "0.0011529545\n",
            "0.0011980084\n",
            "0.0012219389\n",
            "0.001198496\n",
            "0.0011731229\n",
            "0.0013717624\n",
            "0.0013815154\n",
            "0.0011520569\n",
            "0.001065855\n",
            "0.000987645\n",
            "0.0009811843\n",
            "0.0010941579\n",
            "0.0010369964\n",
            "0.0013881999\n",
            "0.0009973011\n",
            "0.0012697863\n",
            "0.0010681334\n",
            "0.001003106\n",
            "0.001016242\n",
            "0.0009973057\n",
            "0.0010192993\n",
            "0.0011491948\n",
            "0.000980821\n",
            "0.0009780173\n",
            "0.0009969465\n",
            "0.0010200322\n",
            "0.0011443825\n",
            "0.0009832275\n",
            "0.0011710108\n",
            "0.0010097991\n",
            "0.0011767193\n",
            "0.00089458376\n",
            "0.0008759097\n",
            "0.0010623345\n",
            "0.0009906619\n",
            "0.0010002238\n",
            "0.0010669429\n",
            "0.0011241471\n",
            "0.0009174036\n",
            "0.0010636811\n",
            "0.0009950219\n",
            "0.000966852\n",
            "0.0010703582\n",
            "0.0010682758\n",
            "0.0010095168\n",
            "0.0009477742\n",
            "0.00084987143\n",
            "0.000977746\n",
            "0.0009609658\n",
            "0.0010705299\n",
            "0.0009729538\n",
            "0.00084988505\n",
            "0.0009307725\n",
            "0.00087259174\n",
            "0.001023639\n",
            "0.0008580724\n",
            "0.000834289\n",
            "0.00088081445\n",
            "0.0010694435\n",
            "0.0009584529\n",
            "0.0009845557\n",
            "0.0009785204\n",
            "0.0008417009\n",
            "0.00088512123\n",
            "0.0009309904\n",
            "0.0008318211\n",
            "0.00084629864\n",
            "0.0009322517\n",
            "0.00095516955\n",
            "0.0009179603\n",
            "0.0007257804\n",
            "0.0008667262\n",
            "0.0007966063\n",
            "0.0008888171\n",
            "0.0010896177\n",
            "0.0007182738\n",
            "0.000971033\n",
            "0.0009724384\n",
            "0.0007529828\n",
            "0.00076044677\n",
            "0.0008711687\n",
            "0.0008583463\n",
            "0.0008290304\n",
            "0.0011496921\n",
            "0.0008924488\n",
            "0.0009694903\n",
            "0.00083226315\n",
            "0.0010212013\n",
            "0.0007609622\n",
            "0.00095169526\n",
            "0.0009955607\n",
            "0.00088026613\n",
            "0.00088821835\n",
            "0.000883384\n",
            "0.0008563672\n",
            "0.0008477204\n",
            "0.0008314551\n",
            "0.00097192556\n",
            "0.0009184441\n",
            "0.0007953964\n",
            "0.0007905164\n",
            "0.000785483\n",
            "0.00087183627\n",
            "0.0008716128\n",
            "0.00074311596\n",
            "0.0009785995\n",
            "0.0007688457\n",
            "0.00081069086\n",
            "0.00080328016\n",
            "0.00085038954\n",
            "0.00077578623\n",
            "0.000822216\n",
            "0.0009153574\n",
            "0.00059131277\n",
            "0.0007809209\n",
            "0.00081357494\n",
            "0.0007794568\n",
            "0.0006615483\n",
            "0.0007281143\n",
            "0.0006557665\n",
            "0.0008582496\n",
            "0.0008690669\n",
            "0.0007116099\n",
            "0.0008296545\n",
            "0.00082633656\n",
            "0.0009177603\n",
            "0.0010016313\n",
            "0.0007936462\n",
            "0.0007202424\n",
            "0.000765331\n",
            "0.0008106856\n",
            "0.00085483206\n",
            "0.00072148145\n",
            "0.0007094141\n",
            "0.0007296585\n",
            "0.00077894895\n",
            "0.00066234067\n",
            "0.00081639894\n",
            "0.000680108\n",
            "0.00076814333\n",
            "0.00063856033\n",
            "0.00072076515\n",
            "0.0006783237\n",
            "0.00071139005\n",
            "0.00070125033\n",
            "0.0010077422\n",
            "0.0007687605\n",
            "0.00097398524\n",
            "0.00063944777\n",
            "0.00073998707\n",
            "0.0007258787\n",
            "0.0007457486\n",
            "0.000773901\n",
            "0.0007451247\n",
            "0.000681643\n",
            "0.00061782176\n",
            "0.00068323984\n",
            "0.00062425673\n",
            "0.00076358573\n",
            "0.0008535794\n",
            "0.0007136672\n",
            "0.0007935833\n",
            "0.00062280026\n",
            "0.0006785688\n",
            "0.0007606347\n",
            "0.0005800602\n",
            "0.00078145234\n",
            "0.0006565491\n",
            "0.00068794534\n",
            "0.00061437767\n",
            "0.0007649041\n",
            "0.0006967422\n",
            "0.0008108503\n",
            "0.0008868518\n",
            "0.0006492157\n",
            "0.00072106865\n",
            "0.00072125276\n",
            "0.0006440324\n",
            "0.00065443624\n",
            "0.0007151851\n",
            "0.00069006666\n",
            "0.0005664759\n",
            "0.00060055085\n",
            "0.00067547575\n",
            "0.00068857556\n",
            "0.0006716738\n",
            "0.0006678145\n",
            "0.00070352416\n",
            "0.0006256546\n",
            "0.00076465646\n",
            "0.0007806577\n",
            "0.00073861936\n",
            "0.0006559682\n",
            "0.0008000305\n",
            "0.00078403275\n",
            "0.0008049427\n",
            "0.000591649\n",
            "0.00065384526\n",
            "0.00066286506\n",
            "0.000672401\n",
            "0.0006615906\n",
            "0.000660394\n",
            "0.0007120735\n",
            "0.0006171754\n",
            "0.00072120916\n",
            "0.0005262001\n",
            "0.0006287836\n",
            "0.0005069574\n",
            "0.0005369889\n",
            "0.0009007409\n",
            "0.00056126295\n",
            "0.0005499207\n",
            "0.0006169336\n",
            "0.00066656125\n",
            "0.0006338481\n",
            "0.0005270223\n",
            "0.00053085614\n",
            "0.0005529099\n",
            "0.0005999692\n",
            "0.00069099874\n",
            "0.0006163645\n",
            "0.0006249413\n",
            "0.00056635716\n",
            "0.0006315831\n",
            "0.0006040107\n",
            "0.0007302375\n",
            "0.0006332569\n",
            "0.0005196562\n",
            "0.0006656144\n",
            "0.0004486628\n",
            "0.0004787696\n",
            "0.00054065045\n",
            "0.0004729778\n",
            "0.00057322887\n",
            "0.00057980197\n",
            "0.00055267155\n",
            "0.0006898354\n",
            "0.00045577198\n",
            "0.0005520238\n",
            "0.0005243187\n",
            "0.0004393569\n",
            "0.00057761074\n",
            "0.0005801223\n",
            "0.0004722353\n",
            "0.00056397886\n",
            "0.0006104812\n",
            "0.0005864735\n",
            "0.0006065909\n",
            "0.0004931789\n",
            "0.00052287214\n",
            "0.00058356585\n",
            "0.0005530504\n",
            "0.00046232046\n",
            "0.00048000613\n",
            "0.000496233\n",
            "0.0006240881\n",
            "0.00047777014\n",
            "0.0005740131\n",
            "0.000533531\n",
            "0.00045746073\n",
            "0.00050636916\n",
            "0.0004525445\n",
            "0.0005293677\n",
            "0.0005376407\n",
            "0.00045446862\n",
            "0.00049648364\n",
            "0.0007394181\n",
            "0.00054111495\n",
            "0.0005134616\n",
            "0.00041190156\n",
            "0.0004478041\n",
            "0.0005013967\n",
            "0.00055033126\n",
            "0.00043522456\n",
            "0.0005758314\n",
            "0.0005342888\n",
            "0.0004236081\n",
            "0.0005026548\n",
            "0.00045347263\n",
            "0.00049514254\n",
            "0.00051621307\n",
            "0.00048717973\n",
            "0.00040021416\n",
            "0.00045593182\n",
            "0.00047607246\n",
            "0.00054765336\n",
            "0.0004822689\n",
            "0.00057086546\n",
            "0.00050199364\n",
            "0.0005350721\n",
            "0.000488977\n",
            "0.00041667468\n",
            "0.0004911278\n",
            "0.00042248407\n",
            "0.00043543466\n",
            "0.00044875772\n",
            "0.00047233776\n",
            "0.000575372\n",
            "0.0004542969\n",
            "0.0004156077\n",
            "0.0005375396\n",
            "0.00046276455\n",
            "0.00048879196\n",
            "0.0005425872\n",
            "0.0004758993\n",
            "0.00037007453\n",
            "0.000546947\n",
            "0.00045400133\n",
            "0.00049311644\n",
            "0.00039531937\n",
            "0.00051914254\n",
            "0.0004577787\n",
            "0.00044097373\n",
            "0.0005259332\n",
            "0.0004078712\n",
            "0.0003877411\n",
            "0.0004606804\n",
            "0.0004269745\n",
            "0.00045474424\n",
            "0.0004680679\n",
            "0.0004099869\n",
            "0.00039188602\n",
            "0.00039684697\n",
            "0.0003935597\n",
            "0.0004320608\n",
            "0.00040958804\n",
            "0.0003967238\n",
            "0.00039806165\n",
            "0.0006218011\n",
            "0.0004626433\n",
            "0.00050974404\n",
            "0.0006727651\n",
            "0.0004455398\n",
            "0.0004692825\n",
            "0.00045455358\n",
            "0.00042576788\n",
            "0.0004399015\n",
            "0.0004313526\n",
            "0.00041347655\n",
            "0.0003719075\n",
            "0.0003728626\n",
            "0.00043304136\n",
            "0.0004561063\n",
            "0.00044583078\n",
            "0.00043272402\n",
            "0.00038229264\n",
            "0.0004447992\n",
            "0.00041465423\n",
            "0.0004274419\n",
            "0.00039685395\n",
            "0.00044388472\n",
            "0.00042693684\n",
            "0.00043780418\n",
            "0.000578409\n",
            "0.00043160678\n",
            "0.00042096648\n",
            "0.00043156365\n",
            "0.00037959954\n",
            "0.00032955428\n",
            "0.00046330426\n",
            "0.00041699805\n",
            "0.00041084687\n",
            "0.000378163\n",
            "0.00046552927\n",
            "0.00042602557\n",
            "0.00038185465\n",
            "0.0004128048\n",
            "0.00039921893\n",
            "0.00033652503\n",
            "0.00038864228\n",
            "0.00038368924\n",
            "0.000516838\n",
            "0.00033826835\n",
            "0.00031730765\n",
            "0.00031683972\n",
            "0.0003439408\n",
            "0.0003172439\n",
            "0.00041906012\n",
            "0.00034960837\n",
            "0.00032644183\n",
            "0.00045881208\n",
            "0.00030715208\n",
            "0.00033425118\n",
            "0.00036855997\n",
            "0.0003941881\n",
            "0.00035948778\n",
            "0.00034048047\n",
            "0.00035214916\n",
            "0.00062000076\n",
            "0.0003996385\n",
            "0.0003902893\n",
            "0.00037323273\n",
            "0.00043612916\n",
            "0.00040229704\n",
            "0.0004730937\n",
            "0.00041310335\n",
            "0.00034204105\n",
            "0.00036355108\n",
            "0.00032621194\n",
            "0.00031853738\n",
            "0.0003398351\n",
            "0.00034625904\n",
            "0.00040198895\n",
            "0.00039496835\n",
            "0.00032981078\n",
            "0.00036562336\n",
            "0.00028508357\n",
            "0.00029379557\n",
            "0.00042073205\n",
            "0.00038354524\n",
            "0.00036387087\n",
            "0.00035189648\n",
            "0.00041892062\n",
            "0.0003265112\n",
            "0.00035076364\n",
            "0.00040251983\n",
            "0.00034503097\n",
            "0.00038217445\n",
            "0.0003810543\n",
            "0.00034924335\n",
            "0.00043175078\n",
            "0.00030139342\n",
            "0.00036570607\n",
            "0.00038977255\n",
            "0.0003513113\n",
            "0.00037528604\n",
            "0.00038158323\n",
            "0.00031258556\n",
            "0.00035192262\n",
            "0.00037487538\n",
            "0.00038116268\n",
            "0.00036690192\n",
            "0.00034179245\n",
            "0.00038134513\n",
            "0.00037209547\n",
            "0.00031242505\n",
            "0.00035181563\n",
            "0.00036672695\n",
            "0.00042166893\n",
            "0.00031647045\n",
            "0.0003133792\n",
            "0.00037861275\n",
            "0.00033925092\n",
            "0.00033570584\n",
            "0.0003642191\n",
            "0.0003412859\n",
            "0.0003302707\n",
            "0.0003942439\n",
            "0.00030590658\n",
            "0.00037959803\n",
            "0.00037401236\n",
            "0.00031496093\n",
            "0.0004235248\n",
            "0.00029835507\n",
            "0.00029813728\n",
            "0.0003741\n",
            "0.00036539207\n",
            "0.00028704054\n",
            "0.00032321186\n",
            "0.00027489706\n",
            "0.000368607\n",
            "0.0002960635\n",
            "0.00036843753\n",
            "0.00030762007\n",
            "0.00031763216\n",
            "0.0003048608\n",
            "0.00036589234\n",
            "0.00029068702\n",
            "0.0003167286\n",
            "0.00036191964\n",
            "0.0002553915\n",
            "0.00031130298\n",
            "0.0002914032\n",
            "0.00031830443\n",
            "0.00034631963\n",
            "0.0003146005\n",
            "0.00036304936\n",
            "0.00034484328\n",
            "0.0003001276\n",
            "0.00029722808\n",
            "0.00026017166\n",
            "0.00025417455\n",
            "0.00033022542\n",
            "0.00032165728\n",
            "0.00031116148\n",
            "0.00043832167\n",
            "0.00034464002\n",
            "0.00026796202\n",
            "0.00036993017\n",
            "0.00028194793\n",
            "0.00031738324\n",
            "0.000318815\n",
            "0.00024555152\n",
            "0.0002571765\n",
            "0.0003163084\n",
            "0.00026429014\n",
            "0.00030951804\n",
            "0.00029728492\n",
            "0.000230683\n",
            "0.0005522602\n",
            "0.00030083396\n",
            "0.000290774\n",
            "0.0002687513\n",
            "0.0002862265\n",
            "0.00027775348\n",
            "0.00027160777\n",
            "0.00030445613\n",
            "0.00029077355\n",
            "0.00027571464\n",
            "0.00041245492\n",
            "0.00022925404\n",
            "0.00034429607\n",
            "0.00021731996\n",
            "0.00023420529\n",
            "0.0002699277\n",
            "0.00024644082\n",
            "0.00028290373\n",
            "0.00023340138\n",
            "0.000278076\n",
            "0.00030307786\n",
            "0.00027323375\n",
            "0.00025889857\n",
            "0.00029878027\n",
            "0.00027542486\n",
            "0.00022733817\n",
            "0.0002916877\n",
            "0.00028940002\n",
            "0.00031897624\n",
            "0.00029060233\n",
            "0.00025265725\n",
            "0.0002742376\n",
            "0.00034826162\n",
            "0.00025114615\n",
            "0.0002537641\n",
            "0.00028042396\n",
            "0.00024604623\n",
            "0.0002500846\n",
            "0.00021027135\n",
            "0.0002163112\n",
            "0.000302848\n",
            "0.0002429445\n",
            "0.00023673465\n",
            "0.00022780112\n",
            "0.00023000085\n",
            "0.0003055348\n",
            "0.00024059339\n",
            "0.00030477263\n",
            "0.00023729305\n",
            "0.00023785631\n",
            "0.00027218094\n",
            "0.00026524844\n",
            "0.0002667272\n",
            "0.00030460823\n",
            "0.0002854634\n",
            "0.0002756212\n",
            "0.00026778865\n",
            "0.00027129127\n",
            "0.00023417771\n",
            "0.00022135157\n",
            "0.0003106258\n",
            "0.00021567491\n",
            "0.00024269054\n",
            "0.00025419088\n",
            "0.0002520082\n",
            "0.00028960488\n",
            "0.00030794612\n",
            "0.00020110993\n",
            "0.00023640049\n",
            "0.00025958318\n",
            "0.00029135786\n",
            "0.00025524676\n",
            "0.0002652825\n",
            "0.0002697898\n",
            "0.00022007758\n",
            "0.00025847115\n",
            "0.00026426648\n",
            "0.00020252157\n",
            "0.00025044626\n",
            "0.00024356504\n",
            "0.00031930287\n",
            "0.00022103635\n",
            "0.0002396475\n",
            "0.00022445177\n",
            "0.00028694863\n",
            "0.0002461947\n",
            "0.0002838714\n",
            "0.00022070794\n",
            "0.00019158576\n",
            "0.00025829612\n",
            "0.0003508659\n",
            "0.00026072495\n",
            "0.00024203314\n",
            "0.00018908277\n",
            "0.00024096848\n",
            "0.00020347191\n",
            "0.00022767652\n",
            "0.0002670548\n",
            "0.00020830562\n",
            "0.0003063049\n",
            "0.0003816547\n",
            "0.00028385915\n",
            "0.00031815685\n",
            "0.00025589974\n",
            "0.00022713676\n",
            "0.00036306726\n",
            "0.000410167\n",
            "0.00021463404\n",
            "0.0002057387\n",
            "0.00025813683\n",
            "0.00024313913\n",
            "0.00020595832\n",
            "0.00019809885\n",
            "0.00026368385\n",
            "0.00028581553\n",
            "0.00023474962\n",
            "0.000323461\n",
            "0.0002686915\n",
            "0.00020599863\n",
            "0.00024317073\n",
            "0.00024338116\n",
            "0.0002635763\n",
            "0.00018558654\n",
            "0.0002519363\n",
            "0.00026356813\n",
            "0.00024074574\n",
            "0.00021046928\n",
            "0.00023815216\n",
            "0.00022752432\n",
            "0.00018033353\n",
            "0.00021124947\n",
            "0.00033693668\n",
            "0.00021680299\n",
            "0.00022291658\n",
            "0.0002255552\n",
            "0.00019402734\n",
            "0.00031942868\n",
            "0.0002795143\n",
            "0.00019745332\n",
            "0.00020431785\n",
            "0.00030359742\n",
            "0.00018481257\n",
            "0.00020562457\n",
            "0.00027911036\n",
            "0.00023594497\n",
            "0.00028800507\n",
            "0.00017779914\n",
            "0.00024235365\n",
            "0.0002639568\n",
            "0.0002019606\n",
            "0.00019918097\n",
            "0.00023195171\n",
            "0.00019593842\n",
            "0.00017290487\n",
            "0.00019303072\n",
            "0.0002128542\n",
            "0.00020773483\n",
            "0.00022819382\n",
            "0.0002758009\n",
            "0.00019602566\n",
            "0.0002350431\n",
            "0.00023849041\n",
            "0.00017341292\n",
            "0.00020035394\n",
            "0.00019003965\n",
            "0.00017647589\n",
            "0.00015461416\n",
            "0.00016944227\n",
            "0.00021634293\n",
            "0.00022624695\n",
            "0.00023406006\n",
            "0.00018697554\n",
            "0.00025472324\n",
            "0.00016763386\n",
            "0.00017465785\n",
            "0.00018622802\n",
            "0.00015212165\n",
            "0.0001872328\n",
            "0.00023548628\n",
            "0.0001524405\n",
            "0.0003724084\n",
            "0.00017642925\n",
            "0.00015999365\n",
            "0.00023717953\n",
            "0.00018936001\n",
            "0.00016611189\n",
            "0.00017948735\n",
            "0.00018508403\n",
            "0.00017490434\n",
            "0.00018265925\n",
            "0.00017744754\n",
            "0.00019255461\n",
            "0.0001636985\n",
            "0.00015469358\n",
            "0.00016319499\n",
            "0.00024393023\n",
            "0.0001864568\n",
            "0.0002335041\n",
            "0.00016182836\n",
            "0.0001833819\n",
            "0.0002666234\n",
            "0.00018309358\n",
            "0.00018732445\n",
            "0.00018623772\n",
            "0.0001972372\n",
            "0.00021275257\n",
            "0.00017000259\n",
            "0.0002035772\n",
            "0.00013842506\n",
            "0.00016013197\n",
            "0.00014768723\n",
            "0.00023937853\n",
            "0.00015557605\n",
            "0.00016552646\n",
            "0.0001919421\n",
            "0.00027133996\n",
            "0.00015153292\n",
            "0.00019900537\n",
            "0.0002462577\n",
            "0.0003276138\n",
            "0.00030066466\n",
            "0.00015372835\n",
            "0.00014565384\n",
            "0.00018185678\n",
            "0.0003681589\n",
            "0.00032940521\n",
            "0.00020543936\n",
            "0.0001501294\n",
            "0.00019661443\n",
            "0.00020533169\n",
            "0.00018952835\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RNJOZZzAZ3K"
      },
      "source": [
        "def nn_hylleraas(x, params):\n",
        "    r = np.linalg.norm(x, axis=1)\n",
        "    r1 = r[0]\n",
        "    r2 = r[1]\n",
        "\n",
        "    s = r1 + r2\n",
        "    t = r1 - r2\n",
        "    u = np.linalg.norm(np.subtract(x[1], x[0]))\n",
        "    return np.exp(-2*s)*(1 + 0.5*u*np.exp(-u))*predict(x, params)\n",
        "\n",
        "nn_hylleraas_wf = Wavefunction(nn_hylleraas, params)\n",
        "print(nn_hylleraas_wf.p_gradlog_eval(np.array([[2.0, 1.0, 1.0], [1.0, 1.0, 2.0]])))\n",
        "print(hirschfelder_f(np.array([[2.0, 1.0, 1.0], [1.0, 1.0, 2.0]]), [1.013, 0.2119, 0.1406, -0.003]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpdBDmaSA3t6"
      },
      "source": [
        "# I don't like this but i can't think of a more elegant way of evaluating\n",
        "# these operators atm without writing custom code for the ML wavefunction\n",
        "# that unrolls the parameter list\n",
        "\n",
        "@partial(jit, static_argnums=(1,))\n",
        "def sr_op_ml(config, wf):\n",
        "    gradlog = wf.p_gradlog_eval(config)\n",
        "    ih = itime_hamiltonian(config, wf)\n",
        "    \n",
        "    # reuse gradlog to save memory\n",
        "    gradlog = np.concatenate((np.array([1]), np.concatenate(tuple(np.concatenate((glw.flatten(), gb.flatten())) for (glw, gb) in gradlog))))\n",
        "    return np.multiply(ih, gradlog)\n",
        "\n",
        "@partial(jit, static_argnums=(1,))\n",
        "def overlap_matrix_ml(config, wf):\n",
        "    \"\"\"\n",
        "    Find the overlap matrix on the space of the parametric derivatives of `wf`\n",
        "    \"\"\"\n",
        "    \n",
        "    gradlog = wf.p_gradlog_eval(config)\n",
        "    gradlog = np.concatenate((np.array([1]), np.concatenate(tuple(np.concatenate((glw.flatten(), gb.flatten())) for (glw, gb) in gradlog))))\n",
        "    overlap_ij = vmap(lambda idx: gradlog[idx[0]]*gradlog[idx[1]])\n",
        "    \n",
        "    grid_pairs = np.array([(i,j) for i in range(gradlog.shape[0]) for j in range(gradlog.shape[0])])\n",
        "    \n",
        "    return overlap_ij(grid_pairs).reshape(gradlog.shape[0], gradlog.shape[0])"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMncMCk7A4EX"
      },
      "source": [
        "n_equi = 100\n",
        "n_iter = 10000\n",
        "n_chains = 500\n",
        "xis = random.uniform(key, (n_chains, 2, 3))\n",
        "keys = random.split(key, n_chains)\n",
        "configs = run_mcmc(keys, nn_hylleraas_wf, n_iter, n_equi, 0.5, xis)\n",
        "# Using VMAP here causes big time memory issues on devices with low memory\n",
        "# I believe this is because JAX copies the wavefunction parameters to each\n",
        "# vmap thread that is executing -> n_iter*n_chains*len(params) float32s\n",
        "# which quickly runs into the hundreds of GB. what is the workaround for this?\n",
        "# Surely a solved problem?\n",
        "\n",
        "# Regardless we are still vmapping over each n_iter set of configs inside the\n",
        "# monte_carlo function, so we incur n_chains serial executions\n",
        "E_E, E_V = reduce_mc_outs(jax.lax.map(lambda x: monte_carlo(x, local_energy, nn_hylleraas_wf), configs))\n",
        "overlap_E, overlap_V = reduce_mc_outs(jax.lax.map(lambda x: monte_carlo(x, overlap_matrix_ml, nn_hylleraas_wf), configs))\n",
        "sr_E, sr_V = reduce_mc_outs(jax.lax.map(lambda x: monte_carlo(x, sr_op_ml, nn_hylleraas_wf), configs))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqCD9tXtK2oy",
        "outputId": "dc369d9b-17f2-4031-c048-a4363f92eeb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "E_E"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray(-2.8516073, dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvbdgYBBlBFg"
      },
      "source": [
        "from jax.scipy.sparse.linalg import cg\n",
        "layer_sizes = [3, 12, 12, 1]\n",
        "params = list(np.load('good_nn.npy',allow_pickle=True))\n",
        "params = np.concatenate(tuple(np.concatenate((w.flatten(), b.flatten())) for (w, b) in params))\n",
        "p_wrapped = []\n",
        "idx=0\n",
        "for m, n in zip(layer_sizes[:-1], layer_sizes[1:]):\n",
        "    p_wrapped.append(\n",
        "        [params[idx:idx + m*n].reshape((n, m)), params[idx + m*n:idx + (m+1)*(n)]]\n",
        "    )\n",
        "    idx += (m+1)*(n)\n",
        "params = p_wrapped\n",
        "params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ra3ey4tOB3FA",
        "outputId": "c2603356-9b27-41a0-8ff0-d1837a08a20a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "n_equi = 1000\n",
        "n_iter = 10000\n",
        "n_chains = 300\n",
        "xis = random.uniform(key, (n_chains, 2, 3))\n",
        "keys = random.split(key, n_chains+1)\n",
        "ml_wf = Wavefunction(nn_hylleraas, params)\n",
        "\n",
        "p_wrapped = params\n",
        "\n",
        "for i in range(400):\n",
        "  keys = random.split(keys[-1], n_chains+1)\n",
        "  configs = run_mcmc(keys[:-1], ml_wf, n_iter, n_equi, 0.5, xis)\n",
        "  E_E, E_V = reduce_mc_outs(run_int(configs, local_energy, ml_wf))\n",
        "\n",
        "  def odotx(x):\n",
        "      @partial(jit, static_argnums=(1,))\n",
        "      def op(c, w):\n",
        "        gradlog = w.p_gradlog_eval(c)\n",
        "        gradlog = np.concatenate((np.array([1]), np.concatenate(tuple(np.concatenate((glw.flatten(), gb.flatten())) for (glw, gb) in gradlog))))\n",
        "\n",
        "        return np.multiply(gradlog, np.dot(gradlog, x))\n",
        "\n",
        "      E, V = reduce_mc_outs(run_int(configs, op, ml_wf))\n",
        "      return E\n",
        "\n",
        "  sr_E, sr_V = reduce_mc_outs(run_int(configs, sr_op_ml, ml_wf))\n",
        "\n",
        "  dps, _ = cg(odotx, sr_E)\n",
        "  p_flat = np.concatenate(tuple(np.concatenate((w.flatten(), b.flatten())) for (w, b) in p_wrapped))\n",
        "  dps = dps[1:] / dps[0]\n",
        "  p_flat = np.add(p_flat, dps)\n",
        "\n",
        "  sizes = layer_sizes\n",
        "  idx = 0\n",
        "  p_wrapped = []\n",
        "  for m, n in zip(sizes[:-1], sizes[1:]):\n",
        "    p_wrapped.append(\n",
        "        [p_flat[idx:idx + m*n].reshape((n, m)), p_flat[idx + m*n:idx + (m+1)*(n)]]\n",
        "    )\n",
        "    idx += (m+1)*(n)\n",
        "\n",
        "  ml_wf = Wavefunction(nn_hylleraas, p_wrapped)\n",
        "  print(\"{} pm {} at step {}\".format(E_E, np.sqrt(E_V), i))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-2.899165391921997 pm 0.0006035205442458391 at step 0\n",
            "-2.898836135864258 pm 0.0005917842499911785 at step 1\n",
            "-2.8989243507385254 pm 0.0005316136521287262 at step 2\n",
            "-2.8994643688201904 pm 0.0005737557075917721 at step 3\n",
            "-2.899426221847534 pm 0.0005650423699989915 at step 4\n",
            "-2.9001028537750244 pm 0.0005424456321634352 at step 5\n",
            "-2.8991734981536865 pm 0.000598137965425849 at step 6\n",
            "-2.8997559547424316 pm 0.0006672914605587721 at step 7\n",
            "-2.8993923664093018 pm 0.0006329311290755868 at step 8\n",
            "-2.900200843811035 pm 0.0005620394367724657 at step 9\n",
            "-2.9005956649780273 pm 0.0006317804218269885 at step 10\n",
            "-2.900709390640259 pm 0.0006901273736730218 at step 11\n",
            "-2.9003806114196777 pm 0.0006241296650841832 at step 12\n",
            "-2.8994028568267822 pm 0.0005616185371764004 at step 13\n",
            "-2.8997457027435303 pm 0.0005336973699741066 at step 14\n",
            "-2.8989109992980957 pm 0.0005374276079237461 at step 15\n",
            "-2.900068521499634 pm 0.0005583901074714959 at step 16\n",
            "-2.9008495807647705 pm 0.000536436855327338 at step 17\n",
            "-2.89998722076416 pm 0.0005244903150014579 at step 18\n",
            "-2.9002983570098877 pm 0.0005073893698863685 at step 19\n",
            "-2.89996075630188 pm 0.0004929736605845392 at step 20\n",
            "-2.900204658508301 pm 0.0005143402377143502 at step 21\n",
            "-2.8996427059173584 pm 0.0005781155778095126 at step 22\n",
            "-2.900681734085083 pm 0.0005733092548325658 at step 23\n",
            "-2.9002463817596436 pm 0.0005559336859732866 at step 24\n",
            "-2.9008371829986572 pm 0.0005358232301659882 at step 25\n",
            "-2.9003124237060547 pm 0.0005392493330873549 at step 26\n",
            "-2.9000606536865234 pm 0.0005714267608709633 at step 27\n",
            "-2.9006896018981934 pm 0.000497421424370259 at step 28\n",
            "-2.9007067680358887 pm 0.0005218054284341633 at step 29\n",
            "-2.9004971981048584 pm 0.0004813084378838539 at step 30\n",
            "-2.900907516479492 pm 0.0004956477787345648 at step 31\n",
            "-2.9004673957824707 pm 0.0005233503761701286 at step 32\n",
            "-2.9013752937316895 pm 0.0005221620667725801 at step 33\n",
            "-2.9011664390563965 pm 0.000562695728149265 at step 34\n",
            "-2.900947093963623 pm 0.0005416940548457205 at step 35\n",
            "-2.900324583053589 pm 0.0005508176400326192 at step 36\n",
            "-2.90073299407959 pm 0.0005491601186804473 at step 37\n",
            "-2.900150775909424 pm 0.0005608271458186209 at step 38\n",
            "-2.900684356689453 pm 0.0005883036064915359 at step 39\n",
            "-2.9009621143341064 pm 0.0005470364703796804 at step 40\n",
            "-2.9011807441711426 pm 0.0005455486243590713 at step 41\n",
            "-2.900346040725708 pm 0.0004998439108021557 at step 42\n",
            "-2.9003350734710693 pm 0.0005719910841435194 at step 43\n",
            "-2.9013619422912598 pm 0.0005104116862639785 at step 44\n",
            "-2.900761127471924 pm 0.0005051527987234294 at step 45\n",
            "-2.9014732837677 pm 0.0005660785245709121 at step 46\n",
            "-2.900810956954956 pm 0.0004949109279550612 at step 47\n",
            "-2.901339292526245 pm 0.00047890786663629115 at step 48\n",
            "-2.900493621826172 pm 0.0004629828908946365 at step 49\n",
            "-2.9010519981384277 pm 0.0005117139662615955 at step 50\n",
            "-2.90069842338562 pm 0.0004890479613095522 at step 51\n",
            "-2.901376962661743 pm 0.0005328857805579901 at step 52\n",
            "-2.9022364616394043 pm 0.0005972440121695399 at step 53\n",
            "-2.9005048274993896 pm 0.0004829051031265408 at step 54\n",
            "-2.9014172554016113 pm 0.0004940113285556436 at step 55\n",
            "-2.9016053676605225 pm 0.0006969663663767278 at step 56\n",
            "-2.901585578918457 pm 0.00048211420653387904 at step 57\n",
            "-2.9014816284179688 pm 0.0005016386858187616 at step 58\n",
            "-2.901587963104248 pm 0.0004765849153045565 at step 59\n",
            "-2.901603937149048 pm 0.0005198539583943784 at step 60\n",
            "-2.90162992477417 pm 0.0005471224430948496 at step 61\n",
            "-2.9014110565185547 pm 0.0004730307846330106 at step 62\n",
            "-2.901590347290039 pm 0.0005274041905067861 at step 63\n",
            "-2.9009759426116943 pm 0.00044626419548876584 at step 64\n",
            "-2.9012110233306885 pm 0.0004564898554235697 at step 65\n",
            "-2.9018471240997314 pm 0.0005306564271450043 at step 66\n",
            "-2.9028334617614746 pm 0.0005550799542106688 at step 67\n",
            "-2.9023237228393555 pm 0.0004400622274260968 at step 68\n",
            "-2.901956558227539 pm 0.0004359999147709459 at step 69\n",
            "-2.9021220207214355 pm 0.0005475665093399584 at step 70\n",
            "-2.902494430541992 pm 0.0004237824759911746 at step 71\n",
            "-2.902024269104004 pm 0.000496471649967134 at step 72\n",
            "-2.901946783065796 pm 0.0004456311871763319 at step 73\n",
            "-2.902221202850342 pm 0.00039407407166436315 at step 74\n",
            "-2.9021668434143066 pm 0.0004712217196356505 at step 75\n",
            "-2.902228355407715 pm 0.0004179814422968775 at step 76\n",
            "-2.901873826980591 pm 0.00038860278436914086 at step 77\n",
            "-2.901986837387085 pm 0.00041264676838181913 at step 78\n",
            "-2.9019744396209717 pm 0.00039148741052486 at step 79\n",
            "-2.902634620666504 pm 0.00046842126175761223 at step 80\n",
            "-2.90248441696167 pm 0.0006797346868552268 at step 81\n",
            "-2.9022557735443115 pm 0.0003825633320957422 at step 82\n",
            "-2.901932954788208 pm 0.00038120365934446454 at step 83\n",
            "-2.902186632156372 pm 0.00038022728404030204 at step 84\n",
            "-2.9021732807159424 pm 0.00039675444713793695 at step 85\n",
            "-2.9021084308624268 pm 0.0003567971580196172 at step 86\n",
            "-2.902235984802246 pm 0.0004076622426509857 at step 87\n",
            "-2.9024393558502197 pm 0.0004344418703112751 at step 88\n",
            "-2.9024159908294678 pm 0.0003458381397649646 at step 89\n",
            "-2.901765823364258 pm 0.00034038498415611684 at step 90\n",
            "-2.9020705223083496 pm 0.00033910147612914443 at step 91\n",
            "-2.902489185333252 pm 0.00042774478788487613 at step 92\n",
            "-2.902742624282837 pm 0.0005395595799200237 at step 93\n",
            "-2.9024808406829834 pm 0.00040221738163381815 at step 94\n",
            "-2.902303695678711 pm 0.0003843731537926942 at step 95\n",
            "-2.902557134628296 pm 0.00043831998482346535 at step 96\n",
            "-2.9027507305145264 pm 0.0004735499678645283 at step 97\n",
            "-2.9019412994384766 pm 0.00040084420470520854 at step 98\n",
            "-2.902134418487549 pm 0.00046302980626933277 at step 99\n",
            "-2.9020211696624756 pm 0.000451092142611742 at step 100\n",
            "-2.902115821838379 pm 0.0004503315140027553 at step 101\n",
            "-2.902086019515991 pm 0.00046228119754232466 at step 102\n",
            "-2.901864767074585 pm 0.0004978225333616138 at step 103\n",
            "-2.902029514312744 pm 0.0004610567702911794 at step 104\n",
            "-2.90230655670166 pm 0.0004597460210788995 at step 105\n",
            "-2.9020392894744873 pm 0.0005260672769509256 at step 106\n",
            "-2.902373790740967 pm 0.0005007385625503957 at step 107\n",
            "-2.902061700820923 pm 0.000519308727234602 at step 108\n",
            "-2.9017324447631836 pm 0.0005348543054424226 at step 109\n",
            "-2.90206241607666 pm 0.00048103349399752915 at step 110\n",
            "-2.9015963077545166 pm 0.00046995808952488005 at step 111\n",
            "-2.9016997814178467 pm 0.0005180418374948204 at step 112\n",
            "-2.9024665355682373 pm 0.0005462163244374096 at step 113\n",
            "-2.90204119682312 pm 0.0005206950008869171 at step 114\n",
            "-2.902247667312622 pm 0.0004988490254618227 at step 115\n",
            "-2.9019863605499268 pm 0.00047471490688622 at step 116\n",
            "-2.9021849632263184 pm 0.0004527581331785768 at step 117\n",
            "-2.902160406112671 pm 0.0004618901002686471 at step 118\n",
            "-2.9023351669311523 pm 0.0005403185496106744 at step 119\n",
            "-2.9031295776367188 pm 0.0006300132372416556 at step 120\n",
            "-2.9022367000579834 pm 0.0004943674430251122 at step 121\n",
            "-2.9028244018554688 pm 0.0005292596761137247 at step 122\n",
            "-2.9027085304260254 pm 0.0005205202614888549 at step 123\n",
            "-2.90183424949646 pm 0.0004445469530764967 at step 124\n",
            "-2.9024159908294678 pm 0.0004874285077676177 at step 125\n",
            "-2.902186393737793 pm 0.00045448887976817787 at step 126\n",
            "-2.902531385421753 pm 0.0007339356234297156 at step 127\n",
            "-2.9020836353302 pm 0.0004529768484644592 at step 128\n",
            "-2.9023404121398926 pm 0.0004967048880644143 at step 129\n",
            "-2.9016153812408447 pm 0.00043598332558758557 at step 130\n",
            "-2.9024314880371094 pm 0.0005388903664425015 at step 131\n",
            "-2.902722120285034 pm 0.0006137581658549607 at step 132\n",
            "-2.902289867401123 pm 0.0005855752388015389 at step 133\n",
            "-2.902479410171509 pm 0.0005247833905741572 at step 134\n",
            "-2.902709484100342 pm 0.00046316557563841343 at step 135\n",
            "-2.901754379272461 pm 0.0004564460541587323 at step 136\n",
            "-2.9022817611694336 pm 0.0004778201109729707 at step 137\n",
            "-2.9021544456481934 pm 0.00045436155050992966 at step 138\n",
            "-2.9017703533172607 pm 0.0004468037805054337 at step 139\n",
            "-2.902247905731201 pm 0.0004708823107648641 at step 140\n",
            "-2.9026801586151123 pm 0.0005580299184657633 at step 141\n",
            "-2.9022932052612305 pm 0.000468096841359511 at step 142\n",
            "-2.9023754596710205 pm 0.00047351937973871827 at step 143\n",
            "-2.9024126529693604 pm 0.0006025334005244076 at step 144\n",
            "-2.9025988578796387 pm 0.0004726241750176996 at step 145\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-b6f1ab8dd256>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m   \u001b[0msr_E\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr_V\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce_mc_outs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfigs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr_op_ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mml_wf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0mdps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0modotx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr_E\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/api.py\u001b[0m in \u001b[0;36mbatched_fun\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   1218\u001b[0m                               lambda: flatten_axes(\"vmap out_axes\", out_tree(),\n\u001b[1;32m   1219\u001b[0m                                                    out_axes),\n\u001b[0;32m-> 1220\u001b[0;31m                               axis_name=axis_name)\n\u001b[0m\u001b[1;32m   1221\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtree_unflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/interpreters/batching.py\u001b[0m in \u001b[0;36mbatch\u001b[0;34m(fun, in_vals, in_dims, out_dim_dests, axis_name)\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[0;31m# executes a batched version of `fun` following out_dim_dests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m   \u001b[0mbatched_fun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dim_dests\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mbatched_fun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0min_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mlu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformation_with_aux\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/linear_util.py\u001b[0m in \u001b[0;36mcall_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m       \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m       \u001b[0;31m# Some transformations yield from inside context managers, so we have to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/api.py\u001b[0m in \u001b[0;36mf_jitted\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0mbackend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_fun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         donated_invars=donated_invars)\n\u001b[0m\u001b[1;32m    216\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtree_unflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(self, fun, *args, **params)\u001b[0m\n\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1144\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_bind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mcall_bind\u001b[0;34m(primitive, fun, *args, **params)\u001b[0m\n\u001b[1;32m   1133\u001b[0m   \u001b[0mtracers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_raise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mmaybe_new_sublevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_trace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_lower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_todos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_trace_todo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, trace, fun, tracers, params)\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mpost_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_tracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/interpreters/batching.py\u001b[0m in \u001b[0;36mprocess_call\u001b[0;34m(self, call_primitive, f, tracers, params)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m       \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdims_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_subtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m       \u001b[0mvals_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_primitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mBatchTracer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvals_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdims_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(self, fun, *args, **params)\u001b[0m\n\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1144\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_bind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mcall_bind\u001b[0;34m(primitive, fun, *args, **params)\u001b[0m\n\u001b[1;32m   1133\u001b[0m   \u001b[0mtracers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_raise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mmaybe_new_sublevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_trace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_lower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_todos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_trace_todo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, trace, fun, tracers, params)\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mpost_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_tracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mprocess_call\u001b[0;34m(self, primitive, f, tracers, params)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m   \u001b[0mprocess_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/interpreters/xla.py\u001b[0m in \u001b[0;36m_xla_call_impl\u001b[0;34m(fun, device, backend, name, donated_invars, *args)\u001b[0m\n\u001b[1;32m    530\u001b[0m                                *unsafe_map(arg_spec, args))\n\u001b[1;32m    531\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcompiled_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mFloatingPointError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m     print(\"Invalid value encountered in the output of a jit function. \"\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/interpreters/xla.py\u001b[0m in \u001b[0;36m_execute_compiled\u001b[0;34m(compiled, handlers, *args)\u001b[0m\n\u001b[1;32m    760\u001b[0m   \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m   \u001b[0minput_bufs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdevice_put\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m   \u001b[0mout_bufs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_bufs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjax_debug_nans\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcheck_nans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxla_call_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_bufs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_buf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_buf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandlers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_bufs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Resource exhausted: Out of memory while trying to allocate 4380000232 bytes."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbPpjy8VCPtw"
      },
      "source": [
        "params = p_wrapped"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ft-LLCs7rESL"
      },
      "source": [
        "np.save('good_nn', params)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1O0_zqnvIFH",
        "outputId": "be4f6d3e-c72f-4cd5-85af-3ef21c38d54d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "E_E"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray(-2.898688, dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_eiO57V9dX4"
      },
      "source": [
        "np.save"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}