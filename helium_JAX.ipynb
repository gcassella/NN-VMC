{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "helium_JAX.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ry7RIBV1HAMm",
        "wdwtL4kuerSF"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gcassella/NN-VMC/blob/main/helium_JAX.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fm9-6jxUCxO8"
      },
      "source": [
        "import jax.numpy as np\n",
        "import jax\n",
        "from jax import random, grad, jacfwd, jacrev, vmap, jit, pmap\n",
        "from jax.ops import index_add, index_update\n",
        "from functools import partial\n",
        "\n",
        "key = random.PRNGKey(0)\n",
        "key, subkey = random.split(key)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hz1WbP_xCxPJ"
      },
      "source": [
        "# Stochastic reconfiguration w/ Hylleraas wavefunction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMcJkpscE8hT"
      },
      "source": [
        "class Wavefunction():\n",
        "    \"\"\"\n",
        "    Helper class to generate functions to evaluate wavefunction gradients.\n",
        "    If I wrote this notebook again I wouldn't bother with this!! JAX really\n",
        "    wants you to stick to pure functional programming, and rightly so!\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "\n",
        "    hess : Callable[[ndarray], ndarray]\n",
        "        Takes config state x, return Hessian matrix of f(x, p0) \n",
        "        w.r.t x\n",
        "    p_grad : Callable[[ndarray, ndarray], ndarray]\n",
        "        grad(f)(x, p) w.r.t p\n",
        "    p_gradlog : Callable[[ndarray, ndarray], ndarray]\n",
        "        grad(log(f))(x, p) w.r.t p\n",
        "    p_gradlog_eval : Callable[[ndarray], ndarray]\n",
        "        Evaluate p_gradlog(x, p0) for config state x\n",
        "    p_grad_eval : Callable[[ndarray], ndarray]\n",
        "        Evaluate p_grad(x, p0) for config state x\n",
        "    lapl_eval : Callable[[ndarray], ndarray]\n",
        "        Evaluate the trace of hess(x) for config state x, equivalent to\n",
        "        evaluating the laplacian of f w.r.t x\n",
        "    eval : Callable[[ndarray], ndarray]\n",
        "        Evaluate f(x) for config state x\n",
        "    pdf_eval : Callable[[ndarray], ndarray]\n",
        "        Evaluate |f(x)|^2 for config state x\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "\n",
        "    f : Callable[[ndarray, ndarray], ndarray]\n",
        "        Wavefunction f(x, p) for config state x containing the electron\n",
        "        coordinates as a (n_electron, 3) ndarray, and parameter vector p\n",
        "        containing the variational parameters of f.\n",
        "    p0 : ndarray\n",
        "        Variational parameters of f.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, f, p0):\n",
        "        self.f = f\n",
        "        self.p = p0\n",
        "\n",
        "        self.hess = jacfwd(jacrev(lambda x: self.f(x, self.p), 0), 0)\n",
        "        self.p_grad = grad(self.f, 1)\n",
        "        self.p_gradlog = grad(lambda x, p: np.log(self.f(x, p)), 1)\n",
        "\n",
        "        # Cache evaluations to speed up?\n",
        "        self.p_gradlog_eval = jit(lambda x: self.p_gradlog(x, self.p))\n",
        "        self.p_grad_eval = jit(lambda x: self.p_grad(x, self.p))\n",
        "        self.lapl_eval = jit(lambda x: np.trace(self.hess(x).reshape(x.shape[0]*x.shape[1], x.shape[0]*x.shape[1])))\n",
        "        self.eval = jit(lambda x: self.f(x, self.p))\n",
        "        self.pdf_eval = jit(lambda x: np.power(np.abs(self.eval(x)), 2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7vBXf1kCxPV"
      },
      "source": [
        "@jit\n",
        "def hirschfelder_f(x, p):\n",
        "    r = np.linalg.norm(x, axis=1)\n",
        "    r1 = r[0]\n",
        "    r2 = r[1]\n",
        "\n",
        "    s = r1 + r2\n",
        "    t = r1 - r2\n",
        "    u = np.linalg.norm(np.subtract(x[1], x[0]))\n",
        "\n",
        "    return np.exp(-2*s)*(1 + 0.5*u*np.exp(-p[0]*u))*(1 + p[1]*s*u + p[2]*np.power(t, 2) + p[3]*np.power(u, 2))\n",
        "\n",
        "hirschfelder = Wavefunction(hirschfelder_f, np.array([1.0, 0.5, 0.5, -0.1]))\n",
        "\n",
        "@jit\n",
        "def simple_f(x, p):\n",
        "    r = np.linalg.norm(x, axis=1)\n",
        "    r1 = r[0]\n",
        "    r2 = r[1]\n",
        "\n",
        "    return np.exp(-p[0]*(r1 + r2))\n",
        "\n",
        "simple = Wavefunction(simple_f, np.array([2.0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sf66BnnICxPg"
      },
      "source": [
        "@partial(jit, static_argnums=(1,))\n",
        "def config_step(key, wf, config, config_prob, config_idx, step_size):\n",
        "    key, subkey = random.split(key)\n",
        "    move_proposal = random.normal(key, shape=(config.shape[1],))*step_size\n",
        "    proposal = index_add(config, config_idx%config.shape[0], move_proposal)\n",
        "    proposal_prob = wf.pdf_eval(proposal)\n",
        "\n",
        "    uniform = random.uniform(subkey)\n",
        "    accept = uniform < (proposal_prob / config_prob)\n",
        "\n",
        "    new_config = np.where(accept, proposal, config)\n",
        "    config_prob = np.where(accept, proposal_prob, config_prob)\n",
        "    return new_config, config_prob, config_idx+1\n",
        "\n",
        "@partial(jit, static_argnums=(1, 2, 3, 4))\n",
        "def get_configs(key, wf, n_iter, n_equi, step_size, initial_config):\n",
        "    \"\"\"\n",
        "    Carries out Metropolis-Hastings sampling according to the distribution |`wf`|**2.0.\n",
        "    \n",
        "    Performs `n_equi` equilibriation steps and `n_iter` sampling steps.\n",
        "    \"\"\"\n",
        "    \n",
        "    def mh_update(i, state):\n",
        "      key, config, prob, idx = state\n",
        "      _, key = random.split(key)\n",
        "      new_config, new_prob, new_idx = config_step(\n",
        "          key,\n",
        "          wf,\n",
        "          config,\n",
        "          prob,\n",
        "          idx,\n",
        "          step_size\n",
        "      )\n",
        "      return (key, new_config, new_prob, new_idx)\n",
        "\n",
        "    def mh_update_and_store(i, state):\n",
        "      key, config, prob, idx, configs = state\n",
        "      _, key = random.split(key)\n",
        "      new_config, new_prob, new_idx = config_step(\n",
        "          key,\n",
        "          wf,\n",
        "          config,\n",
        "          prob,\n",
        "          idx,\n",
        "          step_size\n",
        "      )\n",
        "      new_configs = index_update(configs, idx, new_config)\n",
        "      return (key, new_config, new_prob, new_idx, new_configs)\n",
        "\n",
        "    prob = wf.pdf_eval(initial_config)\n",
        "    key, config, prob, idx = jax.lax.fori_loop(0, n_equi, mh_update, (key, initial_config, prob, 0))\n",
        "    init_configs = np.zeros((n_iter, *initial_config.shape))\n",
        "    key, config, prob, idx, configs = jax.lax.fori_loop(0, n_iter, mh_update_and_store, (key, config, prob, 0, init_configs))\n",
        "\n",
        "    return configs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oezb8chhx_bl"
      },
      "source": [
        "$$\\newcommand{\\ket}[1]{\\left|{#1}\\right\\rangle}\n",
        "\\newcommand{\\bra}[1]{\\left\\langle{#1}\\right|}$$\n",
        "\n",
        "Putting the doc strings for these operators here so I can use latex math:\n",
        "\n",
        "```\n",
        "itime_hamiltonian(config, wf)\n",
        "```\n",
        "\n",
        "Evaluate the local value of a linear expansion of the imaginary time evolution operator\n",
        "\n",
        "$$\\exp{-\\tau\\hat{H}} \\simeq 1 - \\tau\\hat{H}$$\n",
        "\n",
        "```\n",
        "def sr_op(config, wf)\n",
        "```\n",
        "\n",
        "Evaluate the local value of the operator\n",
        "\n",
        "$$\\bra{\\Psi}\\frac{\\partial\\log(\\Psi)}{\\partial\\theta_k}(1-\\tau\\hat{H})\\ket{\\Psi}$$\n",
        "\n",
        "which projects the state $\\ket{\\Psi}$, evolved by the linearly expanded time\n",
        "evolution operator $(1 - \\tau\\hat{H})$, onto the tangent space of the variational parameter basis to obtain the gradient of energy w.r.t variational parameters (i.e. what direction is the Euclidean steepest descent toward the ground state in the parameter space). [Neuscamman 2012]\n",
        "\n",
        "```\n",
        "def overlap_matrix(config, wf)\n",
        "```\n",
        "\n",
        "Evaluate the local value of the overlap matrix of the basis of the variational parameter tangent space $\\bra{\\Psi^i}\\ket{\\Psi^j}$. This is equal to the metric of the parameter space and allows the natural gradient to be calculated. The natural gradient then gives the 'true' direction of steepest descent, as the basis of the parameter space is not guaranteed to be orthonormal (therefore, the Euclidean gradient does not give the 'true' direction of steepest descent). [Amari 1998] [Sorella 2016]\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaWNad0_CxPr"
      },
      "source": [
        "@partial(jit, static_argnums=(1,))\n",
        "def itime_hamiltonian(config, wf, tau=0.01):\n",
        "    n_electron = config.shape[0]\n",
        "    curr_wf = wf.eval(config)\n",
        "    acc = 0\n",
        "    # Calculate kinetic energy\n",
        "    acc += -0.5*(1/curr_wf)*wf.lapl_eval(config)\n",
        "    # Calculate electron-electron energy\n",
        "    for i in range(n_electron):\n",
        "        for j in range(n_electron):\n",
        "            if i < j:\n",
        "                acc += 1 / np.linalg.norm(np.subtract(config[i], config[j]))\n",
        "\n",
        "    # Calculate electron-nucleus energy, assume z=ne FOR NOW\n",
        "    for i in range(n_electron):\n",
        "        acc -= n_electron / np.linalg.norm(config[i])\n",
        "    # Forget about nucleus - nucleus energy FOR NOW\n",
        "\n",
        "    return 1-tau*acc\n",
        "\n",
        "@partial(jit, static_argnums=(1,))\n",
        "def sr_op(config, wf):\n",
        "    gradlog = np.concatenate((np.array([1]), np.array(wf.p_gradlog_eval(config))))\n",
        "    ih = itime_hamiltonian(config, wf)\n",
        "    \n",
        "    return np.multiply(gradlog, ih)\n",
        "\n",
        "@partial(jit, static_argnums=(1,))\n",
        "def overlap_matrix(config, wf):\n",
        "    gradlog = np.concatenate((np.array([1]), np.array(wf.p_gradlog_eval(config))))\n",
        "    gradlog = np.expand_dims(gradlog, 1)\n",
        "    \n",
        "    return np.dot(gradlog, gradlog.T)\n",
        "\n",
        "@partial(jit, static_argnums=(1,))\n",
        "def local_energy(config, wf):\n",
        "    \"\"\"\n",
        "    Local energy operator. Uses JAX autograd to obtain laplacian for KE.\n",
        "    \"\"\"\n",
        "\n",
        "    n_electron = config.shape[0]\n",
        "    acc = 0\n",
        "    # Calculate kinetic energy\n",
        "    acc += -0.5*(1/wf.eval(config))*wf.lapl_eval(config)\n",
        "    # Calculate electron-electron energy\n",
        "    for i in range(n_electron):\n",
        "        for j in range(n_electron):\n",
        "            if i < j:\n",
        "                acc += 1 / np.linalg.norm(np.subtract(config[i], config[j]))\n",
        "\n",
        "    # Calculate electron-nucleus energy, assume z=ne FOR NOW\n",
        "    for i in range(n_electron):\n",
        "        acc -= n_electron / np.linalg.norm(config[i])\n",
        "\n",
        "    return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGuqs5ZbCxPz"
      },
      "source": [
        "@partial(jit, static_argnums=(1,2,))\n",
        "def monte_carlo(configs, op, wf):\n",
        "    \"\"\"\n",
        "    Performs a Monte Carlo integration using the `configs` walker positions\n",
        "    of the expectation value of `op` for the wavefunction `wf`.\n",
        "\n",
        "    Each MCMC chain is broken into samp_rate blocks which are averaged and\n",
        "    their variance handled to eliminate error due to correlations between\n",
        "    MCMC samples. See Sorella lecture notes.\n",
        "    \n",
        "    Returns the expectation value, variance and a list of the sampled values {O_i}\n",
        "    \"\"\"\n",
        "\n",
        "    samp_rate = 100\n",
        "    walker_values = vmap(lambda config: op(config, wf))(configs)\n",
        "    op_output_shape = walker_values[0].shape\n",
        "    num_blocks = (walker_values.shape[0]//samp_rate)\n",
        "    blocks = walker_values[:samp_rate*(num_blocks)].reshape((num_blocks, samp_rate, *op_output_shape))\n",
        "    k = blocks.shape[0]\n",
        "    block_means = np.mean(blocks, axis=1)\n",
        "    op_expec = np.mean(block_means, axis=0)\n",
        "    op_var = 1/(k*(k-1))*np.sum(np.power(block_means - op_expec, 2), axis=0)\n",
        "    return op_expec, op_var"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvTT8u-Zj_UO"
      },
      "source": [
        "# config generation and monte carlo integral evaluation mapped over n_chains\n",
        "# mcmc walkers\n",
        "run_mcmc = vmap(get_configs, in_axes=(0, None, None, None, None, 0), out_axes=0)\n",
        "run_int = vmap(monte_carlo, in_axes=(0, None, None), out_axes=0)\n",
        "\n",
        "def reduce_mc_outs(outs):\n",
        "    \"\"\"\n",
        "    Calculates the mean and variance over the n_chains mcmc walkers, correctly\n",
        "    preserving the statistics\n",
        "    \"\"\"\n",
        "    \n",
        "    k = outs[0].shape[0]\n",
        "    mean = np.mean(outs[0], axis=0)\n",
        "    variance = (1/k/(k-1))*np.sum(outs[1] + np.power(outs[0] - mean, 2), axis=0)\n",
        "    return mean, variance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dxuEJZH6rQM"
      },
      "source": [
        "n_equi = 1000\n",
        "n_iter = 10000\n",
        "n_chains = 500\n",
        "xis = random.uniform(key, (n_chains, 2, 3))\n",
        "keys = random.split(key, n_chains)\n",
        "configs = run_mcmc(keys, simple, n_iter, n_equi, 0.5, xis)\n",
        "E_E, E_V = reduce_mc_outs(run_int(configs, local_energy, simple))\n",
        "overlap_E, overlap_V = reduce_mc_outs(run_int(configs, overlap_matrix, simple))\n",
        "#sr_E, sr_V = reduce_mc_outs(run_int(configs, sr_op, simple))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fRgxAz1DYKN",
        "outputId": "70f3fe1c-118e-4f13-a9e6-04e0354ff169",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "E_E"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray(-2.7496672, dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ry7RIBV1HAMm"
      },
      "source": [
        "## Simple WF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6V9vc3hiC-Qy",
        "outputId": "78e7d689-8df0-48e3-95c6-288f1f218cc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "source": [
        "key = random.PRNGKey(0)\n",
        "n_equi = 10000\n",
        "n_iter = 100000\n",
        "n_chains = 500\n",
        "xis = random.uniform(key, (n_chains, 2, 3))\n",
        "keys = random.split(key, n_chains)\n",
        "simple = Wavefunction(simple_f, np.array([2.0]))\n",
        "vals = [np.array(2.0)]\n",
        "\n",
        "for i in range(40):\n",
        "  configs = run_mcmc(keys, simple, n_iter, n_equi, 0.5, xis)\n",
        "  E_E, E_V = reduce_mc_outs(run_int(configs, local_energy, simple))\n",
        "  overlap_E, overlap_V = reduce_mc_outs(run_int(configs, overlap_matrix, simple))\n",
        "  sr_E, sr_V = reduce_mc_outs(run_int(configs, sr_op, simple))\n",
        "\n",
        "  dps = np.linalg.solve(overlap_E, sr_E)\n",
        "  p0 = np.add(simple.p, dps[1:] / dps[0])\n",
        "  # VERY IMPORTANT NOTE: JAX will not re-jit the operators if Wavefunction.p\n",
        "  # is updated internally by, e.g., a getter or setter. I don't know how to solve\n",
        "  # this problem currently aside from simply reinstantiating Wavefunction each\n",
        "  # time Wavefunction.p needs to be changed\n",
        "  #\n",
        "  # Perhaps this isn't such an issue if one sticks with a purely functional style\n",
        "  # and uses classes like immutable structs?\n",
        "  simple = Wavefunction(simple_f, p0)\n",
        "  vals.append(p0)\n",
        "  print(p0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.9276963]\n",
            "[1.8775854]\n",
            "[1.840691]\n",
            "[1.8123969]\n",
            "[1.7903057]\n",
            "[1.7726845]\n",
            "[1.7584919]\n",
            "[1.7468067]\n",
            "[1.7373891]\n",
            "[1.7295324]\n",
            "[1.7229042]\n",
            "[1.7173197]\n",
            "[1.712626]\n",
            "[1.7087704]\n",
            "[1.7055061]\n",
            "[1.702796]\n",
            "[1.700402]\n",
            "[1.6984478]\n",
            "[1.696852]\n",
            "[1.6954029]\n",
            "[1.6942947]\n",
            "[1.6933272]\n",
            "[1.6924536]\n",
            "[1.6917834]\n",
            "[1.6911279]\n",
            "[1.690638]\n",
            "[1.6901832]\n",
            "[1.6898696]\n",
            "[1.6895639]\n",
            "[1.689298]\n",
            "[1.6891017]\n",
            "[1.6889309]\n",
            "[1.6887196]\n",
            "[1.6885333]\n",
            "[1.6884184]\n",
            "[1.6882296]\n",
            "[1.6881112]\n",
            "[1.6879984]\n",
            "[1.6878452]\n",
            "[1.6877065]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTMPooDtDb12",
        "outputId": "b9f667c9-1d5c-4dd1-bfbf-b6c59bed393d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "configs = run_mcmc(keys, simple, n_iter, n_equi, 0.5, xis)\n",
        "E_E, E_V = reduce_mc_outs(run_int(configs, local_energy, simple))\n",
        "print(\"Ground state energy {} pm {} after 20 iterations with parameter {}\".format(E_E, np.sqrt(E_V), p0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ground state energy -2.8464293479919434 pm 0.0006964870844967663 after 20 iterations with parameter [1.6877065]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mj6TQsqMYhAv"
      },
      "source": [
        "For reference the true minimum of $\\langle E\\rangle \\simeq -2.85 \\text{a. u.}$ of the simple wf ansatz occurs at $\\alpha$=1.6875"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdwtL4kuerSF"
      },
      "source": [
        "## Hirschfelder wavefunction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xhnt3LSetLC",
        "outputId": "bfeefb88-6f2c-4133-d103-c21e528bd91e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "source": [
        "n_equi = 10000\n",
        "n_iter = 100000\n",
        "n_chains = 100\n",
        "xis = random.uniform(key, (n_chains, 2, 3))\n",
        "keys = random.split(key, n_chains)\n",
        "hirschfelder = Wavefunction(hirschfelder_f, np.array([1.0, 0.5, 0.5, -0.1]))\n",
        "vals = [p0]\n",
        "\n",
        "for i in range(40):\n",
        "  configs = run_mcmc(keys, hirschfelder, n_iter, n_equi, 0.5, xis)\n",
        "  E_E, E_V = reduce_mc_outs(run_int(configs, local_energy, hirschfelder))\n",
        "  overlap_E, overlap_V = reduce_mc_outs(run_int(configs, overlap_matrix, hirschfelder))\n",
        "  sr_E, sr_V = reduce_mc_outs(run_int(configs, sr_op, hirschfelder))\n",
        "\n",
        "  dps = np.linalg.solve(overlap_E, sr_E)\n",
        "  p0 = np.add(hirschfelder.p, dps[1:] / dps[0])\n",
        "  hirschfelder = Wavefunction(hirschfelder_f, p0)\n",
        "  vals.append(p0)\n",
        "  print(p0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 1.4236624   0.5324303   0.44780415 -0.1185777 ]\n",
            "[ 1.8274268   0.52555925  0.38857707 -0.14034285]\n",
            "[ 2.1532216   0.5014799   0.34327382 -0.14871532]\n",
            "[ 2.1948583   0.47658455  0.31286922 -0.14467369]\n",
            "[ 2.0115626   0.4537761   0.29153362 -0.13413884]\n",
            "[ 1.8770361   0.43206456  0.27365276 -0.12360602]\n",
            "[ 1.7592784   0.41211852  0.2586345  -0.11351466]\n",
            "[ 1.6649686   0.39407298  0.24570781 -0.10441782]\n",
            "[ 1.577496    0.37777448  0.23474768 -0.09616143]\n",
            "[ 1.5076677   0.3633409   0.22492754 -0.0890992 ]\n",
            "[ 1.4467105   0.35067847  0.21633679 -0.08313622]\n",
            "[ 1.3945475   0.33940798  0.2087992  -0.0780477 ]\n",
            "[ 1.3490429   0.32962883  0.20208855 -0.07381274]\n",
            "[ 1.3063763   0.3208327   0.19627199 -0.07010238]\n",
            "[ 1.2701937   0.31305918  0.1910933  -0.06695889]\n",
            "[ 1.2391235   0.3063077   0.18647681 -0.06442851]\n",
            "[ 1.2104979   0.30031046  0.18252607 -0.06231608]\n",
            "[ 1.187672    0.29506224  0.17885162 -0.06052488]\n",
            "[ 1.1666455   0.29016247  0.17578799 -0.05881443]\n",
            "[ 1.1456738   0.28589332  0.17293528 -0.0574354 ]\n",
            "[ 1.129263    0.28224057  0.17024308 -0.05635386]\n",
            "[ 1.1131905   0.27873302  0.16814847 -0.05524873]\n",
            "[ 1.0984658   0.27568334  0.1661749  -0.05435653]\n",
            "[ 1.0862108   0.2730122   0.16440171 -0.05358232]\n",
            "[ 1.0750093   0.27056193  0.16283806 -0.05288792]\n",
            "[ 1.0650601   0.26846287  0.16138089 -0.05233345]\n",
            "[ 1.0556324   0.26661938  0.16012001 -0.05189104]\n",
            "[ 1.049433    0.2649742   0.1589145  -0.05145911]\n",
            "[ 1.042331    0.26350895  0.15793398 -0.05111194]\n",
            "[ 1.0345564   0.262147    0.15706775 -0.05081553]\n",
            "[ 1.0291275   0.26089907  0.15629807 -0.05046355]\n",
            "[ 1.0244983   0.2598552   0.15557402 -0.0502286 ]\n",
            "[ 1.019854    0.2589599   0.15485553 -0.05004753]\n",
            "[ 1.0158389   0.25807545  0.1542707  -0.0498124 ]\n",
            "[ 1.0135514   0.25729942  0.1538427  -0.04958712]\n",
            "[ 1.010198    0.2565669   0.1534436  -0.04938283]\n",
            "[ 1.0065552   0.2558841   0.1530831  -0.04922125]\n",
            "[ 1.0040314   0.2554026   0.15273264 -0.04915685]\n",
            "[ 1.0028024   0.25498846  0.15240397 -0.04907146]\n",
            "[ 0.9998273   0.2546163   0.1520364  -0.04904639]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gr6FvtZFe7pu",
        "outputId": "cc805b0b-30b7-4e89-db4e-d3452a02ce91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "configs = run_mcmc(keys, hirschfelder, n_iter, n_equi, 0.5, xis)\n",
        "E_E, E_V = reduce_mc_outs(run_int(configs, local_energy, hirschfelder))\n",
        "print(\"Ground state energy {} pm {} after 40 iterations with parameter {}\".format(E_E, np.sqrt(E_V), p0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ground state energy -2.901717185974121 pm 0.00027340053929947317 after 40 iterations with parameter [ 0.9998273   0.2546163   0.1520364  -0.04904639]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKAYSNCrjTfO",
        "outputId": "98be4685-e9ec-4f1d-f1e5-e0bc83ef90b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"This {}mHa from the true ground state energy\".format(\n",
        "    np.abs(-2.903 - E_E)*1e3\n",
        "))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This 1.2829303741455078mHa from the true ground state energy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ampdYuMD_vB1"
      },
      "source": [
        "## Hirschfelder-type wavefunction with NNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYIUOcbIAI6c"
      },
      "source": [
        "$$\\psi_\\theta(\\vec{x_1}, \\vec{x_2})=e^{-2\\left(r_{1}+r_{2}\\right)}\\left(1+\\frac{1}{2} r_{12} e^{-r_{12}}\\right) g_\\theta\\left(r_{1}, r_{2}, r_{12}\\right)$$\n",
        "\n",
        "where\n",
        "\n",
        "$$r_i = |\\vec{x_i}|,$$\n",
        "$$r_{ij} = |\\vec{x_i} - \\vec{x_j}|,$$\n",
        "\n",
        "$\\vec{x_i}$ is the coordinate of the $i$th electron, $\\theta$ is the variational parameter vector and $g_\\theta$ is a neural network with 3 inputs (the Hylleraas coordinates $\\{r_i\\}$ and $\\{r_{ij}\\}$)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZg7Ko5E_uPB"
      },
      "source": [
        "# A helper function to randomly initialize weights and biases\n",
        "# for a dense neural network layer\n",
        "def random_layer_params(m, n, key, scale=1):\n",
        "  w_key, b_key = random.split(key)\n",
        "  return [scale * random.normal(w_key, (n, m)), scale * random.normal(b_key, (n,))]\n",
        "\n",
        "# Initialize all layers for a fully-connected neural network with sizes \"sizes\"\n",
        "def init_network_params(sizes, key):\n",
        "  keys = random.split(key, len(sizes))\n",
        "  return [random_layer_params(m, n, k) for m, n, k in zip(sizes[:-1], sizes[1:], keys)]\n",
        "\n",
        "@jit\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def predict(x, params):\n",
        "  # per-example predictions\n",
        "  r = np.linalg.norm(x, axis=1)\n",
        "  r1 = r[0]\n",
        "  r2 = r[1]\n",
        "  u = np.linalg.norm(np.subtract(x[1], x[0]))\n",
        "\n",
        "  activations = np.array([r1, r2, u])\n",
        "  for w, b in params[:-1]:\n",
        "    outputs = np.dot(w, activations) + b\n",
        "    activations = tanh(outputs)\n",
        "  \n",
        "  final_w, final_b = params[-1]\n",
        "  outputs = np.dot(final_w, activations) + final_b\n",
        "  return outputs[0]\n",
        " \n",
        "layer_sizes = [3, 12, 12, 1]\n",
        "key, subkey = random.split(key)\n",
        "params = init_network_params(layer_sizes, key)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpwNLUTxL8gH"
      },
      "source": [
        "Pretrain params to match $g(r1,r2,r12) = \\left(1+0.2119 s u+0.1406 t^{2}-0.003 u^{2}\\right)$ using JAX built in ADAM optimizer and the loss function\n",
        "\n",
        "$$L_\\theta = \\frac{1}{N}\\sum_{n=0}^{N}(g(\\vec{x}^{(n)}_1, \\vec{x}^{(n)}_2) - g_\\theta(\\vec{x}^{(n)}_1, \\vec{x}^{(n)}_2))^2$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "np8F70iuMFSl",
        "outputId": "34d720f2-e473-4a53-b470-315f353452f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "def g(x):\n",
        "  r = np.linalg.norm(x, axis=1)\n",
        "  r1 = r[0]\n",
        "  r2 = r[1]\n",
        "\n",
        "  s = r1 + r2\n",
        "  t = r1 - r2\n",
        "  u = np.linalg.norm(np.subtract(x[1], x[0]))\n",
        "  return (1 + 0.2119*s*u + 0.1406*t**2.0 - 0.003*u**2.0)\n",
        "\n",
        "g(np.array([[2.0, 1.0, 1.0], [1.0,1.0,2.0]]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray(2.4620862, dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GKewXcjR5qI"
      },
      "source": [
        "from jax.experimental import optimizers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkuaUei8R9T0",
        "outputId": "247d0382-daad-4767-8fc1-8a5603e28448",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "batch_size = 1000\n",
        "opt_init, opt_update, get_params = optimizers.adam(step_size=1e-2)\n",
        "\n",
        "def loss(params, inputs, targets):\n",
        "    # Computes average loss for the batch\n",
        "    predictions = vmap(predict, in_axes=(0, None))(inputs, params)\n",
        "    return np.mean((targets - predictions)**2.0)\n",
        "\n",
        "opt_state = opt_init(params)\n",
        "\n",
        "@jit\n",
        "def step(i, opt_state, x1, y1):\n",
        "    p = get_params(opt_state)\n",
        "    v, g = jax.value_and_grad(loss)(p, x1, y1)\n",
        "    return v, opt_update(i, g, opt_state)\n",
        "\n",
        "for i in range(1000):\n",
        "  key, subkey = jax.random.split(key)\n",
        "  xis = random.uniform(key, (batch_size, 2, 3))\n",
        "  yis = vmap(g)(xis)\n",
        "             \n",
        "  v, opt_state = step(i, opt_state, xis, yis)\n",
        "  print(v)\n",
        "\n",
        "params = get_params(opt_state)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9622067\n",
            "0.57115227\n",
            "0.6202285\n",
            "0.5641536\n",
            "0.5099169\n",
            "0.35819966\n",
            "0.27679816\n",
            "0.27328286\n",
            "0.29929578\n",
            "0.25216475\n",
            "0.219412\n",
            "0.1625536\n",
            "0.15355973\n",
            "0.11677519\n",
            "0.10389205\n",
            "0.10031447\n",
            "0.093164496\n",
            "0.0881365\n",
            "0.07747473\n",
            "0.05603502\n",
            "0.041122366\n",
            "0.041176233\n",
            "0.040185124\n",
            "0.03940032\n",
            "0.03855387\n",
            "0.033953447\n",
            "0.032044124\n",
            "0.02646026\n",
            "0.023414023\n",
            "0.024791678\n",
            "0.024816183\n",
            "0.027833462\n",
            "0.025229642\n",
            "0.021677772\n",
            "0.023617515\n",
            "0.01920466\n",
            "0.020640142\n",
            "0.021336377\n",
            "0.023836236\n",
            "0.021429835\n",
            "0.020081155\n",
            "0.018534623\n",
            "0.018833501\n",
            "0.017895047\n",
            "0.016987296\n",
            "0.020895587\n",
            "0.018035853\n",
            "0.018343316\n",
            "0.017376853\n",
            "0.017063968\n",
            "0.015897049\n",
            "0.016556155\n",
            "0.01710117\n",
            "0.016812699\n",
            "0.015962474\n",
            "0.014042572\n",
            "0.014829695\n",
            "0.014093005\n",
            "0.01362275\n",
            "0.013545292\n",
            "0.012196322\n",
            "0.013471211\n",
            "0.0128430985\n",
            "0.012355731\n",
            "0.011464764\n",
            "0.013038701\n",
            "0.01049162\n",
            "0.012264994\n",
            "0.011119368\n",
            "0.011652776\n",
            "0.010454827\n",
            "0.011171997\n",
            "0.010785171\n",
            "0.009996867\n",
            "0.009987246\n",
            "0.00907038\n",
            "0.010144171\n",
            "0.009574603\n",
            "0.008720641\n",
            "0.008558175\n",
            "0.008256233\n",
            "0.008788521\n",
            "0.0074349474\n",
            "0.008565261\n",
            "0.008287347\n",
            "0.008347998\n",
            "0.008401364\n",
            "0.006968939\n",
            "0.0076739094\n",
            "0.008005726\n",
            "0.007850067\n",
            "0.0071623004\n",
            "0.0067056254\n",
            "0.006912859\n",
            "0.006605664\n",
            "0.0060326937\n",
            "0.0072360756\n",
            "0.0069444105\n",
            "0.007030161\n",
            "0.0063205827\n",
            "0.005625598\n",
            "0.006212903\n",
            "0.0055278074\n",
            "0.005702661\n",
            "0.005172455\n",
            "0.0052777743\n",
            "0.0055106203\n",
            "0.0052659856\n",
            "0.005717575\n",
            "0.0048306193\n",
            "0.005481834\n",
            "0.0059824022\n",
            "0.0052485503\n",
            "0.0048861783\n",
            "0.0047685737\n",
            "0.0053974926\n",
            "0.00491107\n",
            "0.004217584\n",
            "0.004541788\n",
            "0.0049004927\n",
            "0.0046360022\n",
            "0.004591492\n",
            "0.004517362\n",
            "0.004845962\n",
            "0.0048029977\n",
            "0.004351475\n",
            "0.004402756\n",
            "0.0042265486\n",
            "0.0046330313\n",
            "0.0037961528\n",
            "0.0036182897\n",
            "0.0042055845\n",
            "0.0041781054\n",
            "0.004048579\n",
            "0.003868966\n",
            "0.0034843653\n",
            "0.003581503\n",
            "0.0035276448\n",
            "0.0038859288\n",
            "0.0038331153\n",
            "0.0037707833\n",
            "0.0032937604\n",
            "0.0034181962\n",
            "0.0035896527\n",
            "0.0035497986\n",
            "0.0032996628\n",
            "0.0032862364\n",
            "0.0031965512\n",
            "0.0030205147\n",
            "0.003864175\n",
            "0.0029463528\n",
            "0.0033915779\n",
            "0.0040385947\n",
            "0.002823586\n",
            "0.003043831\n",
            "0.0028234834\n",
            "0.0029613492\n",
            "0.0028709031\n",
            "0.0028951718\n",
            "0.0031240685\n",
            "0.0027658346\n",
            "0.0029383996\n",
            "0.0025689835\n",
            "0.0028800617\n",
            "0.002781295\n",
            "0.0028933564\n",
            "0.0025195933\n",
            "0.0023942825\n",
            "0.0028509554\n",
            "0.0027049733\n",
            "0.0027333086\n",
            "0.0019946822\n",
            "0.0021842362\n",
            "0.002398811\n",
            "0.003004647\n",
            "0.0023742225\n",
            "0.0025500837\n",
            "0.0023281067\n",
            "0.0026863758\n",
            "0.0025322195\n",
            "0.002319153\n",
            "0.0023763038\n",
            "0.0021895547\n",
            "0.0024124442\n",
            "0.0026099172\n",
            "0.0018291227\n",
            "0.0019671826\n",
            "0.002083817\n",
            "0.0020445464\n",
            "0.0022057046\n",
            "0.0018896185\n",
            "0.0020332518\n",
            "0.00186717\n",
            "0.0018299468\n",
            "0.0019377167\n",
            "0.0019521505\n",
            "0.0018561942\n",
            "0.0016993997\n",
            "0.0017807806\n",
            "0.0020728954\n",
            "0.0019590035\n",
            "0.0018160263\n",
            "0.0016632945\n",
            "0.0018997694\n",
            "0.0016784673\n",
            "0.0016581398\n",
            "0.00149101\n",
            "0.0018053473\n",
            "0.001459328\n",
            "0.0018299896\n",
            "0.0015103918\n",
            "0.0015587126\n",
            "0.0018561285\n",
            "0.0019242099\n",
            "0.001865858\n",
            "0.0014936703\n",
            "0.0017356232\n",
            "0.001621593\n",
            "0.001684783\n",
            "0.0014299973\n",
            "0.0014969186\n",
            "0.0017509352\n",
            "0.0017824452\n",
            "0.0014182421\n",
            "0.0015427395\n",
            "0.0017774036\n",
            "0.0014029832\n",
            "0.0015968782\n",
            "0.0016279145\n",
            "0.0018490995\n",
            "0.0016936727\n",
            "0.0012441694\n",
            "0.0014605041\n",
            "0.0015986575\n",
            "0.0010739287\n",
            "0.0013679923\n",
            "0.0012745879\n",
            "0.0015107957\n",
            "0.0014763267\n",
            "0.001261677\n",
            "0.0011868043\n",
            "0.0012797449\n",
            "0.0013356381\n",
            "0.0013043176\n",
            "0.0010749426\n",
            "0.0013610472\n",
            "0.001045551\n",
            "0.0011929718\n",
            "0.001267731\n",
            "0.0012503118\n",
            "0.001208404\n",
            "0.0012172778\n",
            "0.0012330265\n",
            "0.0012253459\n",
            "0.00112599\n",
            "0.00111063\n",
            "0.0013705335\n",
            "0.0012108672\n",
            "0.001247289\n",
            "0.001282614\n",
            "0.0010989372\n",
            "0.0011406705\n",
            "0.0012304788\n",
            "0.001220465\n",
            "0.0012001375\n",
            "0.0010466609\n",
            "0.0010229814\n",
            "0.0014321499\n",
            "0.0010049351\n",
            "0.0012242025\n",
            "0.0011529545\n",
            "0.0011980084\n",
            "0.0012219389\n",
            "0.001198496\n",
            "0.0011731229\n",
            "0.0013717624\n",
            "0.0013815154\n",
            "0.0011520569\n",
            "0.001065855\n",
            "0.000987645\n",
            "0.0009811843\n",
            "0.0010941579\n",
            "0.0010369964\n",
            "0.0013881999\n",
            "0.0009973011\n",
            "0.0012697863\n",
            "0.0010681334\n",
            "0.001003106\n",
            "0.001016242\n",
            "0.0009973057\n",
            "0.0010192993\n",
            "0.0011491948\n",
            "0.000980821\n",
            "0.0009780173\n",
            "0.0009969465\n",
            "0.0010200322\n",
            "0.0011443825\n",
            "0.0009832275\n",
            "0.0011710108\n",
            "0.0010097991\n",
            "0.0011767193\n",
            "0.00089458376\n",
            "0.0008759097\n",
            "0.0010623345\n",
            "0.0009906619\n",
            "0.0010002238\n",
            "0.0010669429\n",
            "0.0011241471\n",
            "0.0009174036\n",
            "0.0010636811\n",
            "0.0009950219\n",
            "0.000966852\n",
            "0.0010703582\n",
            "0.0010682758\n",
            "0.0010095168\n",
            "0.0009477742\n",
            "0.00084987143\n",
            "0.000977746\n",
            "0.0009609658\n",
            "0.0010705299\n",
            "0.0009729538\n",
            "0.00084988505\n",
            "0.0009307725\n",
            "0.00087259174\n",
            "0.001023639\n",
            "0.0008580724\n",
            "0.000834289\n",
            "0.00088081445\n",
            "0.0010694435\n",
            "0.0009584529\n",
            "0.0009845557\n",
            "0.0009785204\n",
            "0.0008417009\n",
            "0.00088512123\n",
            "0.0009309904\n",
            "0.0008318211\n",
            "0.00084629864\n",
            "0.0009322517\n",
            "0.00095516955\n",
            "0.0009179603\n",
            "0.0007257804\n",
            "0.0008667262\n",
            "0.0007966063\n",
            "0.0008888171\n",
            "0.0010896177\n",
            "0.0007182738\n",
            "0.000971033\n",
            "0.0009724384\n",
            "0.0007529828\n",
            "0.00076044677\n",
            "0.0008711687\n",
            "0.0008583463\n",
            "0.0008290304\n",
            "0.0011496921\n",
            "0.0008924488\n",
            "0.0009694903\n",
            "0.00083226315\n",
            "0.0010212013\n",
            "0.0007609622\n",
            "0.00095169526\n",
            "0.0009955607\n",
            "0.00088026613\n",
            "0.00088821835\n",
            "0.000883384\n",
            "0.0008563672\n",
            "0.0008477204\n",
            "0.0008314551\n",
            "0.00097192556\n",
            "0.0009184441\n",
            "0.0007953964\n",
            "0.0007905164\n",
            "0.000785483\n",
            "0.00087183627\n",
            "0.0008716128\n",
            "0.00074311596\n",
            "0.0009785995\n",
            "0.0007688457\n",
            "0.00081069086\n",
            "0.00080328016\n",
            "0.00085038954\n",
            "0.00077578623\n",
            "0.000822216\n",
            "0.0009153574\n",
            "0.00059131277\n",
            "0.0007809209\n",
            "0.00081357494\n",
            "0.0007794568\n",
            "0.0006615483\n",
            "0.0007281143\n",
            "0.0006557665\n",
            "0.0008582496\n",
            "0.0008690669\n",
            "0.0007116099\n",
            "0.0008296545\n",
            "0.00082633656\n",
            "0.0009177603\n",
            "0.0010016313\n",
            "0.0007936462\n",
            "0.0007202424\n",
            "0.000765331\n",
            "0.0008106856\n",
            "0.00085483206\n",
            "0.00072148145\n",
            "0.0007094141\n",
            "0.0007296585\n",
            "0.00077894895\n",
            "0.00066234067\n",
            "0.00081639894\n",
            "0.000680108\n",
            "0.00076814333\n",
            "0.00063856033\n",
            "0.00072076515\n",
            "0.0006783237\n",
            "0.00071139005\n",
            "0.00070125033\n",
            "0.0010077422\n",
            "0.0007687605\n",
            "0.00097398524\n",
            "0.00063944777\n",
            "0.00073998707\n",
            "0.0007258787\n",
            "0.0007457486\n",
            "0.000773901\n",
            "0.0007451247\n",
            "0.000681643\n",
            "0.00061782176\n",
            "0.00068323984\n",
            "0.00062425673\n",
            "0.00076358573\n",
            "0.0008535794\n",
            "0.0007136672\n",
            "0.0007935833\n",
            "0.00062280026\n",
            "0.0006785688\n",
            "0.0007606347\n",
            "0.0005800602\n",
            "0.00078145234\n",
            "0.0006565491\n",
            "0.00068794534\n",
            "0.00061437767\n",
            "0.0007649041\n",
            "0.0006967422\n",
            "0.0008108503\n",
            "0.0008868518\n",
            "0.0006492157\n",
            "0.00072106865\n",
            "0.00072125276\n",
            "0.0006440324\n",
            "0.00065443624\n",
            "0.0007151851\n",
            "0.00069006666\n",
            "0.0005664759\n",
            "0.00060055085\n",
            "0.00067547575\n",
            "0.00068857556\n",
            "0.0006716738\n",
            "0.0006678145\n",
            "0.00070352416\n",
            "0.0006256546\n",
            "0.00076465646\n",
            "0.0007806577\n",
            "0.00073861936\n",
            "0.0006559682\n",
            "0.0008000305\n",
            "0.00078403275\n",
            "0.0008049427\n",
            "0.000591649\n",
            "0.00065384526\n",
            "0.00066286506\n",
            "0.000672401\n",
            "0.0006615906\n",
            "0.000660394\n",
            "0.0007120735\n",
            "0.0006171754\n",
            "0.00072120916\n",
            "0.0005262001\n",
            "0.0006287836\n",
            "0.0005069574\n",
            "0.0005369889\n",
            "0.0009007409\n",
            "0.00056126295\n",
            "0.0005499207\n",
            "0.0006169336\n",
            "0.00066656125\n",
            "0.0006338481\n",
            "0.0005270223\n",
            "0.00053085614\n",
            "0.0005529099\n",
            "0.0005999692\n",
            "0.00069099874\n",
            "0.0006163645\n",
            "0.0006249413\n",
            "0.00056635716\n",
            "0.0006315831\n",
            "0.0006040107\n",
            "0.0007302375\n",
            "0.0006332569\n",
            "0.0005196562\n",
            "0.0006656144\n",
            "0.0004486628\n",
            "0.0004787696\n",
            "0.00054065045\n",
            "0.0004729778\n",
            "0.00057322887\n",
            "0.00057980197\n",
            "0.00055267155\n",
            "0.0006898354\n",
            "0.00045577198\n",
            "0.0005520238\n",
            "0.0005243187\n",
            "0.0004393569\n",
            "0.00057761074\n",
            "0.0005801223\n",
            "0.0004722353\n",
            "0.00056397886\n",
            "0.0006104812\n",
            "0.0005864735\n",
            "0.0006065909\n",
            "0.0004931789\n",
            "0.00052287214\n",
            "0.00058356585\n",
            "0.0005530504\n",
            "0.00046232046\n",
            "0.00048000613\n",
            "0.000496233\n",
            "0.0006240881\n",
            "0.00047777014\n",
            "0.0005740131\n",
            "0.000533531\n",
            "0.00045746073\n",
            "0.00050636916\n",
            "0.0004525445\n",
            "0.0005293677\n",
            "0.0005376407\n",
            "0.00045446862\n",
            "0.00049648364\n",
            "0.0007394181\n",
            "0.00054111495\n",
            "0.0005134616\n",
            "0.00041190156\n",
            "0.0004478041\n",
            "0.0005013967\n",
            "0.00055033126\n",
            "0.00043522456\n",
            "0.0005758314\n",
            "0.0005342888\n",
            "0.0004236081\n",
            "0.0005026548\n",
            "0.00045347263\n",
            "0.00049514254\n",
            "0.00051621307\n",
            "0.00048717973\n",
            "0.00040021416\n",
            "0.00045593182\n",
            "0.00047607246\n",
            "0.00054765336\n",
            "0.0004822689\n",
            "0.00057086546\n",
            "0.00050199364\n",
            "0.0005350721\n",
            "0.000488977\n",
            "0.00041667468\n",
            "0.0004911278\n",
            "0.00042248407\n",
            "0.00043543466\n",
            "0.00044875772\n",
            "0.00047233776\n",
            "0.000575372\n",
            "0.0004542969\n",
            "0.0004156077\n",
            "0.0005375396\n",
            "0.00046276455\n",
            "0.00048879196\n",
            "0.0005425872\n",
            "0.0004758993\n",
            "0.00037007453\n",
            "0.000546947\n",
            "0.00045400133\n",
            "0.00049311644\n",
            "0.00039531937\n",
            "0.00051914254\n",
            "0.0004577787\n",
            "0.00044097373\n",
            "0.0005259332\n",
            "0.0004078712\n",
            "0.0003877411\n",
            "0.0004606804\n",
            "0.0004269745\n",
            "0.00045474424\n",
            "0.0004680679\n",
            "0.0004099869\n",
            "0.00039188602\n",
            "0.00039684697\n",
            "0.0003935597\n",
            "0.0004320608\n",
            "0.00040958804\n",
            "0.0003967238\n",
            "0.00039806165\n",
            "0.0006218011\n",
            "0.0004626433\n",
            "0.00050974404\n",
            "0.0006727651\n",
            "0.0004455398\n",
            "0.0004692825\n",
            "0.00045455358\n",
            "0.00042576788\n",
            "0.0004399015\n",
            "0.0004313526\n",
            "0.00041347655\n",
            "0.0003719075\n",
            "0.0003728626\n",
            "0.00043304136\n",
            "0.0004561063\n",
            "0.00044583078\n",
            "0.00043272402\n",
            "0.00038229264\n",
            "0.0004447992\n",
            "0.00041465423\n",
            "0.0004274419\n",
            "0.00039685395\n",
            "0.00044388472\n",
            "0.00042693684\n",
            "0.00043780418\n",
            "0.000578409\n",
            "0.00043160678\n",
            "0.00042096648\n",
            "0.00043156365\n",
            "0.00037959954\n",
            "0.00032955428\n",
            "0.00046330426\n",
            "0.00041699805\n",
            "0.00041084687\n",
            "0.000378163\n",
            "0.00046552927\n",
            "0.00042602557\n",
            "0.00038185465\n",
            "0.0004128048\n",
            "0.00039921893\n",
            "0.00033652503\n",
            "0.00038864228\n",
            "0.00038368924\n",
            "0.000516838\n",
            "0.00033826835\n",
            "0.00031730765\n",
            "0.00031683972\n",
            "0.0003439408\n",
            "0.0003172439\n",
            "0.00041906012\n",
            "0.00034960837\n",
            "0.00032644183\n",
            "0.00045881208\n",
            "0.00030715208\n",
            "0.00033425118\n",
            "0.00036855997\n",
            "0.0003941881\n",
            "0.00035948778\n",
            "0.00034048047\n",
            "0.00035214916\n",
            "0.00062000076\n",
            "0.0003996385\n",
            "0.0003902893\n",
            "0.00037323273\n",
            "0.00043612916\n",
            "0.00040229704\n",
            "0.0004730937\n",
            "0.00041310335\n",
            "0.00034204105\n",
            "0.00036355108\n",
            "0.00032621194\n",
            "0.00031853738\n",
            "0.0003398351\n",
            "0.00034625904\n",
            "0.00040198895\n",
            "0.00039496835\n",
            "0.00032981078\n",
            "0.00036562336\n",
            "0.00028508357\n",
            "0.00029379557\n",
            "0.00042073205\n",
            "0.00038354524\n",
            "0.00036387087\n",
            "0.00035189648\n",
            "0.00041892062\n",
            "0.0003265112\n",
            "0.00035076364\n",
            "0.00040251983\n",
            "0.00034503097\n",
            "0.00038217445\n",
            "0.0003810543\n",
            "0.00034924335\n",
            "0.00043175078\n",
            "0.00030139342\n",
            "0.00036570607\n",
            "0.00038977255\n",
            "0.0003513113\n",
            "0.00037528604\n",
            "0.00038158323\n",
            "0.00031258556\n",
            "0.00035192262\n",
            "0.00037487538\n",
            "0.00038116268\n",
            "0.00036690192\n",
            "0.00034179245\n",
            "0.00038134513\n",
            "0.00037209547\n",
            "0.00031242505\n",
            "0.00035181563\n",
            "0.00036672695\n",
            "0.00042166893\n",
            "0.00031647045\n",
            "0.0003133792\n",
            "0.00037861275\n",
            "0.00033925092\n",
            "0.00033570584\n",
            "0.0003642191\n",
            "0.0003412859\n",
            "0.0003302707\n",
            "0.0003942439\n",
            "0.00030590658\n",
            "0.00037959803\n",
            "0.00037401236\n",
            "0.00031496093\n",
            "0.0004235248\n",
            "0.00029835507\n",
            "0.00029813728\n",
            "0.0003741\n",
            "0.00036539207\n",
            "0.00028704054\n",
            "0.00032321186\n",
            "0.00027489706\n",
            "0.000368607\n",
            "0.0002960635\n",
            "0.00036843753\n",
            "0.00030762007\n",
            "0.00031763216\n",
            "0.0003048608\n",
            "0.00036589234\n",
            "0.00029068702\n",
            "0.0003167286\n",
            "0.00036191964\n",
            "0.0002553915\n",
            "0.00031130298\n",
            "0.0002914032\n",
            "0.00031830443\n",
            "0.00034631963\n",
            "0.0003146005\n",
            "0.00036304936\n",
            "0.00034484328\n",
            "0.0003001276\n",
            "0.00029722808\n",
            "0.00026017166\n",
            "0.00025417455\n",
            "0.00033022542\n",
            "0.00032165728\n",
            "0.00031116148\n",
            "0.00043832167\n",
            "0.00034464002\n",
            "0.00026796202\n",
            "0.00036993017\n",
            "0.00028194793\n",
            "0.00031738324\n",
            "0.000318815\n",
            "0.00024555152\n",
            "0.0002571765\n",
            "0.0003163084\n",
            "0.00026429014\n",
            "0.00030951804\n",
            "0.00029728492\n",
            "0.000230683\n",
            "0.0005522602\n",
            "0.00030083396\n",
            "0.000290774\n",
            "0.0002687513\n",
            "0.0002862265\n",
            "0.00027775348\n",
            "0.00027160777\n",
            "0.00030445613\n",
            "0.00029077355\n",
            "0.00027571464\n",
            "0.00041245492\n",
            "0.00022925404\n",
            "0.00034429607\n",
            "0.00021731996\n",
            "0.00023420529\n",
            "0.0002699277\n",
            "0.00024644082\n",
            "0.00028290373\n",
            "0.00023340138\n",
            "0.000278076\n",
            "0.00030307786\n",
            "0.00027323375\n",
            "0.00025889857\n",
            "0.00029878027\n",
            "0.00027542486\n",
            "0.00022733817\n",
            "0.0002916877\n",
            "0.00028940002\n",
            "0.00031897624\n",
            "0.00029060233\n",
            "0.00025265725\n",
            "0.0002742376\n",
            "0.00034826162\n",
            "0.00025114615\n",
            "0.0002537641\n",
            "0.00028042396\n",
            "0.00024604623\n",
            "0.0002500846\n",
            "0.00021027135\n",
            "0.0002163112\n",
            "0.000302848\n",
            "0.0002429445\n",
            "0.00023673465\n",
            "0.00022780112\n",
            "0.00023000085\n",
            "0.0003055348\n",
            "0.00024059339\n",
            "0.00030477263\n",
            "0.00023729305\n",
            "0.00023785631\n",
            "0.00027218094\n",
            "0.00026524844\n",
            "0.0002667272\n",
            "0.00030460823\n",
            "0.0002854634\n",
            "0.0002756212\n",
            "0.00026778865\n",
            "0.00027129127\n",
            "0.00023417771\n",
            "0.00022135157\n",
            "0.0003106258\n",
            "0.00021567491\n",
            "0.00024269054\n",
            "0.00025419088\n",
            "0.0002520082\n",
            "0.00028960488\n",
            "0.00030794612\n",
            "0.00020110993\n",
            "0.00023640049\n",
            "0.00025958318\n",
            "0.00029135786\n",
            "0.00025524676\n",
            "0.0002652825\n",
            "0.0002697898\n",
            "0.00022007758\n",
            "0.00025847115\n",
            "0.00026426648\n",
            "0.00020252157\n",
            "0.00025044626\n",
            "0.00024356504\n",
            "0.00031930287\n",
            "0.00022103635\n",
            "0.0002396475\n",
            "0.00022445177\n",
            "0.00028694863\n",
            "0.0002461947\n",
            "0.0002838714\n",
            "0.00022070794\n",
            "0.00019158576\n",
            "0.00025829612\n",
            "0.0003508659\n",
            "0.00026072495\n",
            "0.00024203314\n",
            "0.00018908277\n",
            "0.00024096848\n",
            "0.00020347191\n",
            "0.00022767652\n",
            "0.0002670548\n",
            "0.00020830562\n",
            "0.0003063049\n",
            "0.0003816547\n",
            "0.00028385915\n",
            "0.00031815685\n",
            "0.00025589974\n",
            "0.00022713676\n",
            "0.00036306726\n",
            "0.000410167\n",
            "0.00021463404\n",
            "0.0002057387\n",
            "0.00025813683\n",
            "0.00024313913\n",
            "0.00020595832\n",
            "0.00019809885\n",
            "0.00026368385\n",
            "0.00028581553\n",
            "0.00023474962\n",
            "0.000323461\n",
            "0.0002686915\n",
            "0.00020599863\n",
            "0.00024317073\n",
            "0.00024338116\n",
            "0.0002635763\n",
            "0.00018558654\n",
            "0.0002519363\n",
            "0.00026356813\n",
            "0.00024074574\n",
            "0.00021046928\n",
            "0.00023815216\n",
            "0.00022752432\n",
            "0.00018033353\n",
            "0.00021124947\n",
            "0.00033693668\n",
            "0.00021680299\n",
            "0.00022291658\n",
            "0.0002255552\n",
            "0.00019402734\n",
            "0.00031942868\n",
            "0.0002795143\n",
            "0.00019745332\n",
            "0.00020431785\n",
            "0.00030359742\n",
            "0.00018481257\n",
            "0.00020562457\n",
            "0.00027911036\n",
            "0.00023594497\n",
            "0.00028800507\n",
            "0.00017779914\n",
            "0.00024235365\n",
            "0.0002639568\n",
            "0.0002019606\n",
            "0.00019918097\n",
            "0.00023195171\n",
            "0.00019593842\n",
            "0.00017290487\n",
            "0.00019303072\n",
            "0.0002128542\n",
            "0.00020773483\n",
            "0.00022819382\n",
            "0.0002758009\n",
            "0.00019602566\n",
            "0.0002350431\n",
            "0.00023849041\n",
            "0.00017341292\n",
            "0.00020035394\n",
            "0.00019003965\n",
            "0.00017647589\n",
            "0.00015461416\n",
            "0.00016944227\n",
            "0.00021634293\n",
            "0.00022624695\n",
            "0.00023406006\n",
            "0.00018697554\n",
            "0.00025472324\n",
            "0.00016763386\n",
            "0.00017465785\n",
            "0.00018622802\n",
            "0.00015212165\n",
            "0.0001872328\n",
            "0.00023548628\n",
            "0.0001524405\n",
            "0.0003724084\n",
            "0.00017642925\n",
            "0.00015999365\n",
            "0.00023717953\n",
            "0.00018936001\n",
            "0.00016611189\n",
            "0.00017948735\n",
            "0.00018508403\n",
            "0.00017490434\n",
            "0.00018265925\n",
            "0.00017744754\n",
            "0.00019255461\n",
            "0.0001636985\n",
            "0.00015469358\n",
            "0.00016319499\n",
            "0.00024393023\n",
            "0.0001864568\n",
            "0.0002335041\n",
            "0.00016182836\n",
            "0.0001833819\n",
            "0.0002666234\n",
            "0.00018309358\n",
            "0.00018732445\n",
            "0.00018623772\n",
            "0.0001972372\n",
            "0.00021275257\n",
            "0.00017000259\n",
            "0.0002035772\n",
            "0.00013842506\n",
            "0.00016013197\n",
            "0.00014768723\n",
            "0.00023937853\n",
            "0.00015557605\n",
            "0.00016552646\n",
            "0.0001919421\n",
            "0.00027133996\n",
            "0.00015153292\n",
            "0.00019900537\n",
            "0.0002462577\n",
            "0.0003276138\n",
            "0.00030066466\n",
            "0.00015372835\n",
            "0.00014565384\n",
            "0.00018185678\n",
            "0.0003681589\n",
            "0.00032940521\n",
            "0.00020543936\n",
            "0.0001501294\n",
            "0.00019661443\n",
            "0.00020533169\n",
            "0.00018952835\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RNJOZZzAZ3K"
      },
      "source": [
        "def nn_hylleraas(x, params):\n",
        "    r = np.linalg.norm(x, axis=1)\n",
        "    r1 = r[0]\n",
        "    r2 = r[1]\n",
        "\n",
        "    s = r1 + r2\n",
        "    t = r1 - r2\n",
        "    u = np.linalg.norm(np.subtract(x[1], x[0]))\n",
        "    return np.exp(-2*s)*(1 + 0.5*u*np.exp(-u))*predict(x, params)\n",
        "\n",
        "nn_hylleraas_wf = Wavefunction(nn_hylleraas, params)\n",
        "print(nn_hylleraas_wf.p_gradlog_eval(np.array([[2.0, 1.0, 1.0], [1.0, 1.0, 2.0]])))\n",
        "print(hirschfelder_f(np.array([[2.0, 1.0, 1.0], [1.0, 1.0, 2.0]]), [1.013, 0.2119, 0.1406, -0.003]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpdBDmaSA3t6"
      },
      "source": [
        "# I don't like this but i can't think of a more elegant way of evaluating\n",
        "# these operators atm without writing custom code for the ML wavefunction\n",
        "# that unrolls the parameter list\n",
        "\n",
        "@partial(jit, static_argnums=(1,))\n",
        "def sr_op_ml(config, wf):\n",
        "    gradlog = wf.p_gradlog_eval(config)\n",
        "    ih = itime_hamiltonian(config, wf)\n",
        "    \n",
        "    # reuse gradlog to save memory\n",
        "    gradlog = np.concatenate((np.array([1]), np.concatenate(tuple(np.concatenate((glw.flatten(), gb.flatten())) for (glw, gb) in gradlog))))\n",
        "    return np.multiply(ih, gradlog)\n",
        "\n",
        "\n",
        "# I don't actually use this method now that I'm using Conjugate Gradient\n",
        "@partial(jit, static_argnums=(1,))\n",
        "def overlap_matrix_ml(config, wf):\n",
        "    \"\"\"\n",
        "    Find the overlap matrix on the space of the parametric derivatives of `wf`\n",
        "    \"\"\"\n",
        "    \n",
        "    gradlog = wf.p_gradlog_eval(config)\n",
        "    gradlog = np.concatenate((np.array([1]), np.concatenate(tuple(np.concatenate((glw.flatten(), gb.flatten())) for (glw, gb) in gradlog))))\n",
        "    overlap_ij = vmap(lambda idx: gradlog[idx[0]]*gradlog[idx[1]])\n",
        "    \n",
        "    grid_pairs = np.array([(i,j) for i in range(gradlog.shape[0]) for j in range(gradlog.shape[0])])\n",
        "    \n",
        "    return overlap_ij(grid_pairs).reshape(gradlog.shape[0], gradlog.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMncMCk7A4EX"
      },
      "source": [
        "n_equi = 100\n",
        "n_iter = 10000\n",
        "n_chains = 500\n",
        "xis = random.uniform(key, (n_chains, 2, 3))\n",
        "keys = random.split(key, n_chains)\n",
        "configs = run_mcmc(keys, nn_hylleraas_wf, n_iter, n_equi, 0.5, xis)\n",
        "# Using VMAP here causes big time memory issues on devices with low memory\n",
        "# I believe this is because JAX copies the wavefunction parameters to each\n",
        "# vmap thread that is executing -> n_iter*n_chains*len(params) float32s\n",
        "# which quickly runs into the hundreds of GB. what is the workaround for this?\n",
        "# Surely a solved problem?\n",
        "\n",
        "# Regardless we are still vmapping over each n_iter set of configs inside the\n",
        "# monte_carlo function, so we incur n_chains serial executions\n",
        "E_E, E_V = reduce_mc_outs(jax.lax.map(lambda x: monte_carlo(x, local_energy, nn_hylleraas_wf), configs))\n",
        "overlap_E, overlap_V = reduce_mc_outs(jax.lax.map(lambda x: monte_carlo(x, overlap_matrix_ml, nn_hylleraas_wf), configs))\n",
        "sr_E, sr_V = reduce_mc_outs(jax.lax.map(lambda x: monte_carlo(x, sr_op_ml, nn_hylleraas_wf), configs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqCD9tXtK2oy",
        "outputId": "dc369d9b-17f2-4031-c048-a4363f92eeb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "E_E"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray(-2.8516073, dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTRLtQX4259S"
      },
      "source": [
        "The conjugate gradient method is used to avoid twofold problems in solving the system\n",
        "\n",
        "$$\\bra{\\Psi}\\frac{\\partial\\log(\\Psi)}{\\partial\\theta_i}(1-\\tau\\hat{H})\\ket{\\Psi} = \\sum_j \\bra{\\Psi^i}\\ket{\\Psi^j}x_j.$$\n",
        "\n",
        "Firstly, building the matrix\n",
        "\n",
        "$$S_{ij} = \\bra{\\Psi^i}\\ket{\\Psi^j}$$\n",
        "\n",
        "requires $O(\\text{num_samples}\\cdot\\text{num_params}^2)$ time and memory to compute. In theory, evaluating the matrix-vector product\n",
        "\n",
        "$$\\sum_j \\bra{\\Psi^i}\\ket{\\Psi^j}x_j$$\n",
        "\n",
        "can be carried out in $O(\\text{num_samples}\\cdot\\text{num_params})$ time and memory. Using the conjugate gradient method means only this product is required, and the full matrix need not be evaluated. Unfortunately my current implementation of the stochastic gradient procedure is bad, so I'm not sure we save on time (although we would in the scaling limit), however the memory saved by not having to fully evaluate and store $S_{ij}$ means we can run more walkers / iterations before the GPU gives up the ghost due to memory limitations. This means we don't have to resort to any serial execution, which means we can feasibly run more walkers faster, which means we can do a better optimization - yay! [Neuscamman 2012]\n",
        "\n",
        "Secondly, $S_{ij}$ has, in my experience so far, almost always been ill conditioned (at least when $\\theta$ is far from the ground state). This makes solving the system above by matrix inversion prone to incredibly large numerical errors, such that optimization always almost failed. For reasons that I currently haven't looked into, the conjugate gradient method doesn't suffer as badly when handling nearly-singular matrices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvbdgYBBlBFg"
      },
      "source": [
        "# Cell used to load NN parameters saved to disk periodically when the device\n",
        "# runs out of memory and crashes the runtime. Haven't bothered looking for\n",
        "# the memory leak yet.\n",
        "\n",
        "from jax.scipy.sparse.linalg import cg\n",
        "layer_sizes = [3, 12, 12, 1]\n",
        "params = list(np.load('good_nn.npy',allow_pickle=True))\n",
        "params = np.concatenate(tuple(np.concatenate((w.flatten(), b.flatten())) for (w, b) in params))\n",
        "p_wrapped = []\n",
        "idx=0\n",
        "for m, n in zip(layer_sizes[:-1], layer_sizes[1:]):\n",
        "    p_wrapped.append(\n",
        "        [params[idx:idx + m*n].reshape((n, m)), params[idx + m*n:idx + (m+1)*(n)]]\n",
        "    )\n",
        "    idx += (m+1)*(n)\n",
        "params = p_wrapped\n",
        "params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ra3ey4tOB3FA",
        "outputId": "c2603356-9b27-41a0-8ff0-d1837a08a20a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "n_equi = 1000\n",
        "n_iter = 10000\n",
        "n_chains = 300\n",
        "xis = random.uniform(key, (n_chains, 2, 3))\n",
        "keys = random.split(key, n_chains+1)\n",
        "ml_wf = Wavefunction(nn_hylleraas, params)\n",
        "\n",
        "p_wrapped = params\n",
        "\n",
        "for i in range(400):\n",
        "  keys = random.split(keys[-1], n_chains+1)\n",
        "  configs = run_mcmc(keys[:-1], ml_wf, n_iter, n_equi, 0.5, xis)\n",
        "  E_E, E_V = reduce_mc_outs(run_int(configs, local_energy, ml_wf))\n",
        "\n",
        "  def odotx(x):\n",
        "      \"\"\"\n",
        "      Calculates the value of $S_{ij}\\cdot x_j$ stochastically. \n",
        "      \n",
        "      This is VERY inefficient, because we don't store the stochastic evaluations \n",
        "      of the gradlogs inbetween evaluations. However, with the interface that JAX\n",
        "      uses for conjugate gradient, I can't currently think of cute way of\n",
        "      doing this (I can think of some very messy ways).\n",
        "      \"\"\"\n",
        "\n",
        "      @partial(jit, static_argnums=(1,))\n",
        "      def op(c, w):\n",
        "        gradlog = w.p_gradlog_eval(c)\n",
        "        gradlog = np.concatenate((np.array([1]), np.concatenate(tuple(np.concatenate((glw.flatten(), gb.flatten())) for (glw, gb) in gradlog))))\n",
        "\n",
        "        return np.multiply(gradlog, np.dot(gradlog, x))\n",
        "\n",
        "      E, V = reduce_mc_outs(run_int(configs, op, ml_wf))\n",
        "      return E\n",
        "\n",
        "  sr_E, sr_V = reduce_mc_outs(run_int(configs, sr_op_ml, ml_wf))\n",
        "\n",
        "  dps, _ = cg(odotx, sr_E)\n",
        "\n",
        "  # Bit of a rigmarole to flatten / unflatten the NN parameters\n",
        "  p_flat = np.concatenate(tuple(np.concatenate((w.flatten(), b.flatten())) for (w, b) in p_wrapped))\n",
        "  dps = dps[1:] / dps[0]\n",
        "  p_flat = np.add(p_flat, dps)\n",
        "\n",
        "  sizes = layer_sizes\n",
        "  idx = 0\n",
        "  p_wrapped = []\n",
        "  for m, n in zip(sizes[:-1], sizes[1:]):\n",
        "    p_wrapped.append(\n",
        "        [p_flat[idx:idx + m*n].reshape((n, m)), p_flat[idx + m*n:idx + (m+1)*(n)]]\n",
        "    )\n",
        "    idx += (m+1)*(n)\n",
        "\n",
        "  ml_wf = Wavefunction(nn_hylleraas, p_wrapped)\n",
        "  print(\"{} pm {} at step {}\".format(E_E, np.sqrt(E_V), i))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-2.899165391921997 pm 0.0006035205442458391 at step 0\n",
            "-2.898836135864258 pm 0.0005917842499911785 at step 1\n",
            "-2.8989243507385254 pm 0.0005316136521287262 at step 2\n",
            "-2.8994643688201904 pm 0.0005737557075917721 at step 3\n",
            "-2.899426221847534 pm 0.0005650423699989915 at step 4\n",
            "-2.9001028537750244 pm 0.0005424456321634352 at step 5\n",
            "-2.8991734981536865 pm 0.000598137965425849 at step 6\n",
            "-2.8997559547424316 pm 0.0006672914605587721 at step 7\n",
            "-2.8993923664093018 pm 0.0006329311290755868 at step 8\n",
            "-2.900200843811035 pm 0.0005620394367724657 at step 9\n",
            "-2.9005956649780273 pm 0.0006317804218269885 at step 10\n",
            "-2.900709390640259 pm 0.0006901273736730218 at step 11\n",
            "-2.9003806114196777 pm 0.0006241296650841832 at step 12\n",
            "-2.8994028568267822 pm 0.0005616185371764004 at step 13\n",
            "-2.8997457027435303 pm 0.0005336973699741066 at step 14\n",
            "-2.8989109992980957 pm 0.0005374276079237461 at step 15\n",
            "-2.900068521499634 pm 0.0005583901074714959 at step 16\n",
            "-2.9008495807647705 pm 0.000536436855327338 at step 17\n",
            "-2.89998722076416 pm 0.0005244903150014579 at step 18\n",
            "-2.9002983570098877 pm 0.0005073893698863685 at step 19\n",
            "-2.89996075630188 pm 0.0004929736605845392 at step 20\n",
            "-2.900204658508301 pm 0.0005143402377143502 at step 21\n",
            "-2.8996427059173584 pm 0.0005781155778095126 at step 22\n",
            "-2.900681734085083 pm 0.0005733092548325658 at step 23\n",
            "-2.9002463817596436 pm 0.0005559336859732866 at step 24\n",
            "-2.9008371829986572 pm 0.0005358232301659882 at step 25\n",
            "-2.9003124237060547 pm 0.0005392493330873549 at step 26\n",
            "-2.9000606536865234 pm 0.0005714267608709633 at step 27\n",
            "-2.9006896018981934 pm 0.000497421424370259 at step 28\n",
            "-2.9007067680358887 pm 0.0005218054284341633 at step 29\n",
            "-2.9004971981048584 pm 0.0004813084378838539 at step 30\n",
            "-2.900907516479492 pm 0.0004956477787345648 at step 31\n",
            "-2.9004673957824707 pm 0.0005233503761701286 at step 32\n",
            "-2.9013752937316895 pm 0.0005221620667725801 at step 33\n",
            "-2.9011664390563965 pm 0.000562695728149265 at step 34\n",
            "-2.900947093963623 pm 0.0005416940548457205 at step 35\n",
            "-2.900324583053589 pm 0.0005508176400326192 at step 36\n",
            "-2.90073299407959 pm 0.0005491601186804473 at step 37\n",
            "-2.900150775909424 pm 0.0005608271458186209 at step 38\n",
            "-2.900684356689453 pm 0.0005883036064915359 at step 39\n",
            "-2.9009621143341064 pm 0.0005470364703796804 at step 40\n",
            "-2.9011807441711426 pm 0.0005455486243590713 at step 41\n",
            "-2.900346040725708 pm 0.0004998439108021557 at step 42\n",
            "-2.9003350734710693 pm 0.0005719910841435194 at step 43\n",
            "-2.9013619422912598 pm 0.0005104116862639785 at step 44\n",
            "-2.900761127471924 pm 0.0005051527987234294 at step 45\n",
            "-2.9014732837677 pm 0.0005660785245709121 at step 46\n",
            "-2.900810956954956 pm 0.0004949109279550612 at step 47\n",
            "-2.901339292526245 pm 0.00047890786663629115 at step 48\n",
            "-2.900493621826172 pm 0.0004629828908946365 at step 49\n",
            "-2.9010519981384277 pm 0.0005117139662615955 at step 50\n",
            "-2.90069842338562 pm 0.0004890479613095522 at step 51\n",
            "-2.901376962661743 pm 0.0005328857805579901 at step 52\n",
            "-2.9022364616394043 pm 0.0005972440121695399 at step 53\n",
            "-2.9005048274993896 pm 0.0004829051031265408 at step 54\n",
            "-2.9014172554016113 pm 0.0004940113285556436 at step 55\n",
            "-2.9016053676605225 pm 0.0006969663663767278 at step 56\n",
            "-2.901585578918457 pm 0.00048211420653387904 at step 57\n",
            "-2.9014816284179688 pm 0.0005016386858187616 at step 58\n",
            "-2.901587963104248 pm 0.0004765849153045565 at step 59\n",
            "-2.901603937149048 pm 0.0005198539583943784 at step 60\n",
            "-2.90162992477417 pm 0.0005471224430948496 at step 61\n",
            "-2.9014110565185547 pm 0.0004730307846330106 at step 62\n",
            "-2.901590347290039 pm 0.0005274041905067861 at step 63\n",
            "-2.9009759426116943 pm 0.00044626419548876584 at step 64\n",
            "-2.9012110233306885 pm 0.0004564898554235697 at step 65\n",
            "-2.9018471240997314 pm 0.0005306564271450043 at step 66\n",
            "-2.9028334617614746 pm 0.0005550799542106688 at step 67\n",
            "-2.9023237228393555 pm 0.0004400622274260968 at step 68\n",
            "-2.901956558227539 pm 0.0004359999147709459 at step 69\n",
            "-2.9021220207214355 pm 0.0005475665093399584 at step 70\n",
            "-2.902494430541992 pm 0.0004237824759911746 at step 71\n",
            "-2.902024269104004 pm 0.000496471649967134 at step 72\n",
            "-2.901946783065796 pm 0.0004456311871763319 at step 73\n",
            "-2.902221202850342 pm 0.00039407407166436315 at step 74\n",
            "-2.9021668434143066 pm 0.0004712217196356505 at step 75\n",
            "-2.902228355407715 pm 0.0004179814422968775 at step 76\n",
            "-2.901873826980591 pm 0.00038860278436914086 at step 77\n",
            "-2.901986837387085 pm 0.00041264676838181913 at step 78\n",
            "-2.9019744396209717 pm 0.00039148741052486 at step 79\n",
            "-2.902634620666504 pm 0.00046842126175761223 at step 80\n",
            "-2.90248441696167 pm 0.0006797346868552268 at step 81\n",
            "-2.9022557735443115 pm 0.0003825633320957422 at step 82\n",
            "-2.901932954788208 pm 0.00038120365934446454 at step 83\n",
            "-2.902186632156372 pm 0.00038022728404030204 at step 84\n",
            "-2.9021732807159424 pm 0.00039675444713793695 at step 85\n",
            "-2.9021084308624268 pm 0.0003567971580196172 at step 86\n",
            "-2.902235984802246 pm 0.0004076622426509857 at step 87\n",
            "-2.9024393558502197 pm 0.0004344418703112751 at step 88\n",
            "-2.9024159908294678 pm 0.0003458381397649646 at step 89\n",
            "-2.901765823364258 pm 0.00034038498415611684 at step 90\n",
            "-2.9020705223083496 pm 0.00033910147612914443 at step 91\n",
            "-2.902489185333252 pm 0.00042774478788487613 at step 92\n",
            "-2.902742624282837 pm 0.0005395595799200237 at step 93\n",
            "-2.9024808406829834 pm 0.00040221738163381815 at step 94\n",
            "-2.902303695678711 pm 0.0003843731537926942 at step 95\n",
            "-2.902557134628296 pm 0.00043831998482346535 at step 96\n",
            "-2.9027507305145264 pm 0.0004735499678645283 at step 97\n",
            "-2.9019412994384766 pm 0.00040084420470520854 at step 98\n",
            "-2.902134418487549 pm 0.00046302980626933277 at step 99\n",
            "-2.9020211696624756 pm 0.000451092142611742 at step 100\n",
            "-2.902115821838379 pm 0.0004503315140027553 at step 101\n",
            "-2.902086019515991 pm 0.00046228119754232466 at step 102\n",
            "-2.901864767074585 pm 0.0004978225333616138 at step 103\n",
            "-2.902029514312744 pm 0.0004610567702911794 at step 104\n",
            "-2.90230655670166 pm 0.0004597460210788995 at step 105\n",
            "-2.9020392894744873 pm 0.0005260672769509256 at step 106\n",
            "-2.902373790740967 pm 0.0005007385625503957 at step 107\n",
            "-2.902061700820923 pm 0.000519308727234602 at step 108\n",
            "-2.9017324447631836 pm 0.0005348543054424226 at step 109\n",
            "-2.90206241607666 pm 0.00048103349399752915 at step 110\n",
            "-2.9015963077545166 pm 0.00046995808952488005 at step 111\n",
            "-2.9016997814178467 pm 0.0005180418374948204 at step 112\n",
            "-2.9024665355682373 pm 0.0005462163244374096 at step 113\n",
            "-2.90204119682312 pm 0.0005206950008869171 at step 114\n",
            "-2.902247667312622 pm 0.0004988490254618227 at step 115\n",
            "-2.9019863605499268 pm 0.00047471490688622 at step 116\n",
            "-2.9021849632263184 pm 0.0004527581331785768 at step 117\n",
            "-2.902160406112671 pm 0.0004618901002686471 at step 118\n",
            "-2.9023351669311523 pm 0.0005403185496106744 at step 119\n",
            "-2.9031295776367188 pm 0.0006300132372416556 at step 120\n",
            "-2.9022367000579834 pm 0.0004943674430251122 at step 121\n",
            "-2.9028244018554688 pm 0.0005292596761137247 at step 122\n",
            "-2.9027085304260254 pm 0.0005205202614888549 at step 123\n",
            "-2.90183424949646 pm 0.0004445469530764967 at step 124\n",
            "-2.9024159908294678 pm 0.0004874285077676177 at step 125\n",
            "-2.902186393737793 pm 0.00045448887976817787 at step 126\n",
            "-2.902531385421753 pm 0.0007339356234297156 at step 127\n",
            "-2.9020836353302 pm 0.0004529768484644592 at step 128\n",
            "-2.9023404121398926 pm 0.0004967048880644143 at step 129\n",
            "-2.9016153812408447 pm 0.00043598332558758557 at step 130\n",
            "-2.9024314880371094 pm 0.0005388903664425015 at step 131\n",
            "-2.902722120285034 pm 0.0006137581658549607 at step 132\n",
            "-2.902289867401123 pm 0.0005855752388015389 at step 133\n",
            "-2.902479410171509 pm 0.0005247833905741572 at step 134\n",
            "-2.902709484100342 pm 0.00046316557563841343 at step 135\n",
            "-2.901754379272461 pm 0.0004564460541587323 at step 136\n",
            "-2.9022817611694336 pm 0.0004778201109729707 at step 137\n",
            "-2.9021544456481934 pm 0.00045436155050992966 at step 138\n",
            "-2.9017703533172607 pm 0.0004468037805054337 at step 139\n",
            "-2.902247905731201 pm 0.0004708823107648641 at step 140\n",
            "-2.9026801586151123 pm 0.0005580299184657633 at step 141\n",
            "-2.9022932052612305 pm 0.000468096841359511 at step 142\n",
            "-2.9023754596710205 pm 0.00047351937973871827 at step 143\n",
            "-2.9024126529693604 pm 0.0006025334005244076 at step 144\n",
            "-2.9025988578796387 pm 0.0004726241750176996 at step 145\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-b6f1ab8dd256>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m   \u001b[0msr_E\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr_V\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce_mc_outs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfigs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr_op_ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mml_wf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0mdps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0modotx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr_E\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/api.py\u001b[0m in \u001b[0;36mbatched_fun\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   1218\u001b[0m                               lambda: flatten_axes(\"vmap out_axes\", out_tree(),\n\u001b[1;32m   1219\u001b[0m                                                    out_axes),\n\u001b[0;32m-> 1220\u001b[0;31m                               axis_name=axis_name)\n\u001b[0m\u001b[1;32m   1221\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtree_unflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/interpreters/batching.py\u001b[0m in \u001b[0;36mbatch\u001b[0;34m(fun, in_vals, in_dims, out_dim_dests, axis_name)\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[0;31m# executes a batched version of `fun` following out_dim_dests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m   \u001b[0mbatched_fun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dim_dests\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mbatched_fun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0min_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mlu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformation_with_aux\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/linear_util.py\u001b[0m in \u001b[0;36mcall_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m       \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m       \u001b[0;31m# Some transformations yield from inside context managers, so we have to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/api.py\u001b[0m in \u001b[0;36mf_jitted\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0mbackend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_fun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         donated_invars=donated_invars)\n\u001b[0m\u001b[1;32m    216\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtree_unflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(self, fun, *args, **params)\u001b[0m\n\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1144\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_bind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mcall_bind\u001b[0;34m(primitive, fun, *args, **params)\u001b[0m\n\u001b[1;32m   1133\u001b[0m   \u001b[0mtracers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_raise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mmaybe_new_sublevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_trace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_lower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_todos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_trace_todo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, trace, fun, tracers, params)\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mpost_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_tracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/interpreters/batching.py\u001b[0m in \u001b[0;36mprocess_call\u001b[0;34m(self, call_primitive, f, tracers, params)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m       \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdims_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_subtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m       \u001b[0mvals_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_primitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mBatchTracer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvals_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdims_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(self, fun, *args, **params)\u001b[0m\n\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1144\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_bind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mcall_bind\u001b[0;34m(primitive, fun, *args, **params)\u001b[0m\n\u001b[1;32m   1133\u001b[0m   \u001b[0mtracers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_raise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mmaybe_new_sublevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_trace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_lower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_todos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_trace_todo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, trace, fun, tracers, params)\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mpost_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_tracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mprocess_call\u001b[0;34m(self, primitive, f, tracers, params)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m   \u001b[0mprocess_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/interpreters/xla.py\u001b[0m in \u001b[0;36m_xla_call_impl\u001b[0;34m(fun, device, backend, name, donated_invars, *args)\u001b[0m\n\u001b[1;32m    530\u001b[0m                                *unsafe_map(arg_spec, args))\n\u001b[1;32m    531\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcompiled_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mFloatingPointError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m     print(\"Invalid value encountered in the output of a jit function. \"\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jax/interpreters/xla.py\u001b[0m in \u001b[0;36m_execute_compiled\u001b[0;34m(compiled, handlers, *args)\u001b[0m\n\u001b[1;32m    760\u001b[0m   \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m   \u001b[0minput_bufs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdevice_put\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m   \u001b[0mout_bufs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_bufs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjax_debug_nans\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcheck_nans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxla_call_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_bufs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_buf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_buf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandlers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_bufs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Resource exhausted: Out of memory while trying to allocate 4380000232 bytes."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImJTRg_M6EtI"
      },
      "source": [
        "The cell above contains an execution / implementation of the stochastic reconfiguration + conjugate gradient optimization of a two hidden layer (12 units each) multi layer perceptron Hylleraas-type wavefunction for the Helium atom. As you can see, the optimization achieves chemical accuracy. \n",
        "\n",
        "The cell was re-executed several times due to out of memory crashes, so the 'step number' is misleading. The true number of optimization steps is near 300."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbPpjy8VCPtw"
      },
      "source": [
        "params = p_wrapped"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ft-LLCs7rESL"
      },
      "source": [
        "np.save('good_nn', params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_eiO57V9dX4"
      },
      "source": [
        "np.save"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NPZaNDx7HSm"
      },
      "source": [
        "# Bibliography\n",
        "\n",
        "S. Amari, Natural Gradient Works Efciently in Learning, Neural Computation 10, 251276 p. 36, 1998.\n",
        "\n",
        "E. Neuscamman, C. J. Umrigar, and G. K.-L. Chan, Optimizing large parameter sets in variational quantum Monte Carlo, Phys. Rev. B, vol. 85, no. 4, p. 045103, 2012.\n",
        "\n",
        "S. Sorella and F. Becca, SISSA Lecture notes on Numerical methods for strongly correlated electrons, p. 147, 2016.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0ThtXxt7oYJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}